
@article{dietterich_hierarchical_2000,
	title = {Hierarchical {Reinforcement} {Learning} with the {MAXQ} {Value} {Function} {Decomposition}},
	volume = {13},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/10266},
	doi = {10.1613/jair.639},
	language = {en-US},
	urldate = {2018-11-30},
	journal = {Journal of Artificial Intelligence Research},
	author = {Dietterich, T. G.},
	month = nov,
	year = {2000},
	pages = {227--303},
	file = {Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/5EZNNUQT/Dietterich - 2000 - Hierarchical Reinforcement Learning with the MAXQ .pdf:application/pdf;Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/XNNGWW5J/10266.html:text/html}
}

@article{barto_recent_2003,
	title = {Recent {Advances} in {Hierarchical} {Reinforcement} {Learning}},
	volume = {13},
	issn = {1573-7594},
	url = {https://doi.org/10.1023/A:1022140919877},
	doi = {10.1023/A:1022140919877},
	abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
	language = {en},
	number = {1},
	urldate = {2018-11-30},
	journal = {Discrete Event Dynamic Systems},
	author = {Barto, Andrew G. and Mahadevan, Sridhar},
	month = jan,
	year = {2003},
	keywords = {Control Architecture, Hierarchical Organization, Learning Algorithm, Recent Attempt, Reinforcement Learning},
	pages = {41--77},
	file = {Barto and Mahadevan - 2003 - Recent Advances in Hierarchical Reinforcement Lear.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/W3MVI3RM/Barto and Mahadevan - 2003 - Recent Advances in Hierarchical Reinforcement Lear.pdf:application/pdf}
}

@article{sutton_between_1999,
	title = {Between {MDPs} and semi-{MDPs}: {A} framework for temporal abstraction in reinforcement learning},
	volume = {112},
	issn = {0004-3702},
	shorttitle = {Between {MDPs} and semi-{MDPs}},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370299000521},
	doi = {10.1016/S0004-3702(99)00052-1},
	abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.},
	number = {1},
	urldate = {2018-11-30},
	journal = {Artificial Intelligence},
	author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
	month = aug,
	year = {1999},
	keywords = {Reinforcement learning, Hierarchical planning, Intra-option learning, Macroactions, Macros, Markov decision processes, Options, Semi-Markov decision processes, Subgoals, Temporal abstraction},
	pages = {181--211},
	file = {ScienceDirect Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/2VNH329F/Sutton et al. - 1999 - Between MDPs and semi-MDPs A framework for tempor.pdf:application/pdf;ScienceDirect Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/U2VVW5AY/S0004370299000521.html:text/html}
}

@incollection{parr_reinforcement_1998,
	title = {Reinforcement {Learning} with {Hierarchies} of {Machines}},
	url = {http://papers.nips.cc/paper/1384-reinforcement-learning-with-hierarchies-of-machines.pdf},
	urldate = {2018-11-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 10},
	publisher = {MIT Press},
	author = {Parr, Ronald and Russell, Stuart J.},
	editor = {Jordan, M. I. and Kearns, M. J. and Solla, S. A.},
	year = {1998},
	pages = {1043--1049},
	file = {NIPS Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/6DZQ33YI/Parr and Russell - 1998 - Reinforcement Learning with Hierarchies of Machine.pdf:application/pdf;NIPS Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/PJ9BTX5M/1384-reinforcement-learning-with-hierarchies-of-machines.html:text/html}
}

@inproceedings{hauskrecht_hierarchical_1998,
	address = {San Francisco, CA, USA},
	series = {{UAI}'98},
	title = {Hierarchical {Solution} of {Markov} {Decision} {Processes} {Using} {Macro}-actions},
	isbn = {978-1-55860-555-8},
	url = {http://dl.acm.org/citation.cfm?id=2074094.2074120},
	abstract = {We investigate the use of temporally abstract actions, or macro-actions, in the solution of Markov decision processes. Unlike current models that combine both primitive actions and macro-actions and leave the state space unchanged, we propose a hierarchical model (using an abstract MDP) that works with macro-actions only, and that significantly reduces the size of the state space. This is achieved by treating macroactions as local policies that act in certain regions of state space, and by restricting states in the abstract MDP to those at the boundaries of regions. The abstract MDP approximates the original and can be solved more efficiently. We discuss several ways in which macro-actions can be generated to ensure good solution quality. Finally, we consider ways in which macro-actions can be reused to solve multiple, related MDPs; and we show that this can justify the computational overhead of macro-action generation.},
	urldate = {2019-01-01},
	booktitle = {Proceedings of the {Fourteenth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Hauskrecht, Milos and Meuleau, Nicolas and Kaelbling, Leslie Pack and Dean, Thomas and Boutilier, Craig},
	year = {1998},
	pages = {220--229},
	file = {arXiv\:1301.7381 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/244R4LG4/Hauskrecht et al. - 2013 - Hierarchical Solution of Markov Decision Processes.pdf:application/pdf}
}

@inproceedings{parr_flexible_1998,
	address = {San Francisco, CA, USA},
	series = {{UAI}'98},
	title = {Flexible {Decomposition} {Algorithms} for {Weakly} {Coupled} {Markov} {Decision} {Problems}},
	isbn = {978-1-55860-555-8},
	url = {http://dl.acm.org/citation.cfm?id=2074094.2074144},
	abstract = {This paper presents two new approaches to decomposing and solving large Markov decision problems (MDPs), a partial decoupling method and a complete decoupling method. In these approaches, a large, stochastic decision problem is divided into smaller pieces. The first approach builds a cache of policies for each part of the problem independently, and then combines the pieces in a separate, light-weight step. A second approach also divides the problem into smaller pieces, but information is communicated between the different problem pieces, allowing intelligent decisions to be made about which piece requires the most attention. Both approaches can be used to find optimal policies or approximately optimal policies with provable bounds. These algorithms also provide a framework for the efficient transfer of knowledge across problems that share similar structure.},
	urldate = {2019-01-01},
	booktitle = {Proceedings of the {Fourteenth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Parr, Ronald},
	year = {1998},
	pages = {422--430},
	file = {arXiv\:1301.7405 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/JUCMTEA3/Parr - 2013 - Flexible Decomposition Algorithms for Weakly Coupl.pdf:application/pdf}
}

@inproceedings{makar_hierarchical_2001,
	address = {New York, NY, USA},
	series = {{AGENTS} '01},
	title = {Hierarchical {Multi}-agent {Reinforcement} {Learning}},
	isbn = {978-1-58113-326-4},
	url = {http://doi.acm.org/10.1145/375735.376302},
	doi = {10.1145/375735.376302},
	abstract = {In this paper we investigate the use of hierarchical reinforcement learning to speed up the acquisition of cooperative multi-agent tasks. We extend the MAXQ framework to the multi-agent case. Each agent uses the same MAXQ hierarchy to decompose a task into sub-tasks. Learning is decentralized, with each agent learning three interrelated skills: how to perform subtasks, which order to do them in, and how to coordinate with other agents. Coordination skills among agents are learned by using joint actions at the highest level(s) of the hierarchy. The Q nodes at the highest level(s) of the hierarchy are configured to represent the joint task-action space among multiple agents. In this approach, each agent only knows what other agents are doing at the level of sub-tasks, and is unaware of lower level (primitive) actions. This hierarchical approach allows agents to learn coordination faster by sharing information at the level of sub-tasks, rather than attempting to learn coordination taking into account primitive joint state-action values. We apply this hierarchical multi-agent reinforcement learning algorithm to a complex AGV scheduling task and compare its performance and speed with other learning approaches, including flat multi-agent, single agent using MAXQ, selfish multiple agents using MAXQ (where each agent acts independently without communicating with the other agents), as well as several well-known AGV heuristics like "first come first serve", "highest queue first" and "nearest station first". We also compare the tradeoffs in learning speed vs. performance of modeling joint action values at multiple levels in the MAXQ hierarchy.},
	urldate = {2019-01-01},
	booktitle = {Proceedings of the {Fifth} {International} {Conference} on {Autonomous} {Agents}},
	publisher = {ACM},
	author = {Makar, Rajbala and Mahadevan, Sridhar and Ghavamzadeh, Mohammad},
	year = {2001},
	pages = {246--253},
	file = {Submitted Version:/Users/guillaume/Documents/Scolaire/Zotero/storage/YA5NESPE/Makar et al. - 2001 - Hierarchical Multi-agent Reinforcement Learning.pdf:application/pdf}
}

@inproceedings{simsek_using_2004,
	address = {New York, NY, USA},
	series = {{ICML} '04},
	title = {Using {Relative} {Novelty} to {Identify} {Useful} {Temporal} {Abstractions} in {Reinforcement} {Learning}},
	isbn = {978-1-58113-838-2},
	url = {http://doi.acm.org/10.1145/1015330.1015353},
	doi = {10.1145/1015330.1015353},
	abstract = {We present a new method for automatically creating useful temporal abstractions in reinforcement learning. We argue that states that allow the agent to transition to a different region of the state space are useful subgoals, and propose a method for identifying them using the concept of relative novelty. When such a state is identified, a temporally-extended activity (e.g., an option) is generated that takes the agent efficiently to this state. We illustrate the utility of the method in a number of tasks.},
	urldate = {2019-01-01},
	booktitle = {Proceedings of the {Twenty}-first {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Şimşek, Özgür and Barto, Andrew G.},
	year = {2004},
	pages = {95--},
	file = {Şimşek and Barto - 2004 - Using relative novelty to identify useful temporal.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/HGQIQHZR/Şimşek and Barto - 2004 - Using relative novelty to identify useful temporal.pdf:application/pdf}
}

@article{fine_hierarchical_1998,
	title = {The {Hierarchical} {Hidden} {Markov} {Model}: {Analysis} and {Applications}},
	volume = {32},
	issn = {1573-0565},
	shorttitle = {The {Hierarchical} {Hidden} {Markov} {Model}},
	url = {https://doi.org/10.1023/A:1007469218079},
	doi = {10.1023/A:1007469218079},
	abstract = {We introduce, analyze and demonstrate a recursive hierarchical generalization of the widely used hidden Markov models, which we name Hierarchical Hidden Markov Models (HHMM). Our model is motivated by the complex multi-scale structure which appears in many natural sequences, particularly in language, handwriting and speech. We seek a systematic unsupervised approach to the modeling of such structures. By extending the standard Baum-Welch (forward-backward) algorithm, we derive an efficient procedure for estimating the model parameters from unlabeled data. We then use the trained model for automatic hierarchical parsing of observation sequences. We describe two applications of our model and its parameter estimation procedure. In the first application we show how to construct hierarchical models of natural English text. In these models different levels of the hierarchy correspond to structures on different length scales in the text. In the second application we demonstrate how HHMMs can be used to automatically identify repeated strokes that represent combination of letters in cursive handwriting.},
	language = {en},
	number = {1},
	urldate = {2019-01-01},
	journal = {Machine Learning},
	author = {Fine, Shai and Singer, Yoram and Tishby, Naftali},
	month = jul,
	year = {1998},
	keywords = {cursive handwriting, hidden variable models, statistical models, temporal pattern recognition},
	pages = {41--62},
	file = {Springer Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/VNZ9ZJID/Fine et al. - 1998 - The Hierarchical Hidden Markov Model Analysis and.pdf:application/pdf}
}

@article{brochu_tutorial_2010,
	title = {A {Tutorial} on {Bayesian} {Optimization} of {Expensive} {Cost} {Functions}, with {Application} to {Active} {User} {Modeling} and {Hierarchical} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1012.2599},
	abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
	urldate = {2019-01-01},
	journal = {arXiv:1012.2599 [cs]},
	author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
	month = dec,
	year = {2010},
	note = {arXiv: 1012.2599},
	keywords = {Computer Science - Machine Learning, G.1.6, G.3, I.2.6},
	file = {arXiv\:1012.2599 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/YAVVS245/Brochu et al. - 2010 - A Tutorial on Bayesian Optimization of Expensive C.pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/X8Q7C82P/1012.html:text/html}
}

@inproceedings{hengst_discovering_2002,
	title = {Discovering {Hierarchy} in {Reinforcement} {Learning} with {HEXQ}},
	abstract = {An open problem in reinforcement learning  is discovering hierarchical structure. HEXQ,  an algorithm which automatically attempts  to decompose and solve a model-free factored  MDP hierarchically is described. By  searching for aliased Markov sub-space regions  based on the state variables the algorithm  uses temporal and state abstraction to  construct a hierarchy of interlinked smaller  MDPs.},
	booktitle = {In {Maching} {Learning}: {Proceedings} of the {Nineteenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann},
	author = {Hengst, Bernhard},
	year = {2002},
	pages = {243--250},
	file = {Citeseer - Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/DLQFG67N/Hengst - 2002 - Discovering Hierarchy in Reinforcement Learning wi.pdf:application/pdf;Citeseer - Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/EMDLIH6L/summary.html:text/html}
}

@inproceedings{dietterich_overview_2000,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {An {Overview} of {MAXQ} {Hierarchical} {Reinforcement} {Learning}},
	isbn = {978-3-540-44914-0},
	abstract = {Reinforcement learning addresses the problem of learning optimal policies for sequential decision-making problems involving stochastic operators and numerical reward functions rather than the more traditional deterministic operators and logical goal predicates. In many ways, reinforcement learning research is recapitulating the development of classical research in planning and problem solving. After studying the problem of solving “flat” problem spaces, researchers have recently turned their attention to hierarchical methods that incorporate subroutines and state abstractions. This paper gives an overview of the MAXQ value function decomposition and its support for state abstraction and action abstraction.},
	language = {en},
	booktitle = {Abstraction, {Reformulation}, and {Approximation}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dietterich, Thomas G.},
	editor = {Choueiry, Berthe Y. and Walsh, Toby},
	year = {2000},
	keywords = {Function Decomposition, Markov Decision Process, Optimal Policy, Primitive Action, Total Reward},
	pages = {26--44},
	file = {Dietterich - 2000 - An Overview of MAXQ Hierarchical Reinforcement Lea.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/II29KGDV/Dietterich - 2000 - An Overview of MAXQ Hierarchical Reinforcement Lea.pdf:application/pdf}
}

@article{bakker_hierarchical_2004,
	title = {Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization: {First} experiments with the {HASSLE} algorithm},
	shorttitle = {Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization},
	abstract = {We introduce a new method for hierarchical reinforcement learning. Highlevelpolicies automatically discover subgoals; low-level policies learn to specializeon different subgoals. Subgoals are represented as desired abstract observations whichcluster raw input data. High-level value functions cover the state space at a coarselevel; low-level value functions cover only parts of the state space at a fine-grainedlevel. Experiments show that this method outperforms several flat...},
	journal = {Proceedings of the 8-th Conference on Intelligent Autonomous Systems},
	author = {Bakker, B and Schmidhuber, J},
	month = jan,
	year = {2004},
	file = {Bakker and Schmidhuber - Hierarchical Reinforcement Learning Based on Subgo.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/VH44V2UA/Bakker and Schmidhuber - Hierarchical Reinforcement Learning Based on Subgo.pdf:application/pdf}
}

@article{nachum_data-efficient_2018,
	title = {Data-{Efficient} {Hierarchical} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1805.08296},
	abstract = {Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and find that it is generally applicable and highly sample-efficient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.},
	urldate = {2019-01-01},
	journal = {arXiv:1805.08296 [cs, stat]},
	author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
	month = may,
	year = {2018},
	note = {arXiv: 1805.08296},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: NIPS 2018},
	file = {arXiv\:1805.08296 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/F5A5LI8N/Nachum et al. - 2018 - Data-Efficient Hierarchical Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/LQCUTPAA/1805.html:text/html}
}

@article{levy_hierarchical_2018,
	title = {Hierarchical {Reinforcement} {Learning} with {Hindsight}},
	url = {http://arxiv.org/abs/1805.08180},
	abstract = {Reinforcement Learning (RL) algorithms can suffer from poor sample efficiency when rewards are delayed and sparse. We introduce a solution that enables agents to learn temporally extended actions at multiple levels of abstraction in a sample efficient and automated fashion. Our approach combines universal value functions and hindsight learning, allowing agents to learn policies belonging to different time scales in parallel. We show that our method significantly accelerates learning in a variety of discrete and continuous tasks.},
	urldate = {2019-01-01},
	journal = {arXiv:1805.08180 [cs, stat]},
	author = {Levy, Andrew and Platt, Robert and Saenko, Kate},
	month = may,
	year = {2018},
	note = {arXiv: 1805.08180},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv\:1805.08180 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/X6BCQDWM/Levy et al. - 2018 - Hierarchical Reinforcement Learning with Hindsight.pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/7MU5DZMP/1805.html:text/html}
}

@incollection{kulkarni_hierarchical_2016,
	title = {Hierarchical {Deep} {Reinforcement} {Learning}: {Integrating} {Temporal} {Abstraction} and {Intrinsic} {Motivation}},
	shorttitle = {Hierarchical {Deep} {Reinforcement} {Learning}},
	url = {http://papers.nips.cc/paper/6233-hierarchical-deep-reinforcement-learning-integrating-temporal-abstraction-and-intrinsic-motivation.pdf},
	urldate = {2019-01-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {3675--3683},
	file = {NIPS Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/3KNCMD4L/Kulkarni et al. - 2016 - Hierarchical Deep Reinforcement Learning Integrat.pdf:application/pdf;NIPS Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/AFTUFS5Q/6233-hierarchical-deep-reinforcement-learning-integrating-temporal-abstraction-and-intrinsic-mo.html:text/html}
}

@article{morimoto_acquisition_2001,
	title = {Acquisition of stand-up behavior by a real robot using hierarchical reinforcement learning},
	volume = {36},
	issn = {0921-8890},
	url = {http://www.sciencedirect.com/science/article/pii/S0921889001001130},
	doi = {10.1016/S0921-8890(01)00113-0},
	abstract = {In this paper, we propose a hierarchical reinforcement learning architecture that realizes practical learning speed in real hardware control tasks. In order to enable learning in a practical number of trials, we introduce a low-dimensional representation of the state of the robot for higher-level planning. The upper level learns a discrete sequence of sub-goals in a low-dimensional state space for achieving the main goal of the task. The lower-level modules learn local trajectories in the original high-dimensional state space to achieve the sub-goal specified by the upper level. We applied the hierarchical architecture to a three-link, two-joint robot for the task of learning to stand up by trial and error. The upper-level learning was implemented by Q-learning, while the lower-level learning was implemented by a continuous actor–critic method. The robot successfully learned to stand up within 750 trials in simulation and then in an additional 170 trials using real hardware. The effects of the setting of the search steps in the upper level and the use of a supplementary reward for achieving sub-goals are also tested in simulation.},
	number = {1},
	urldate = {2019-01-01},
	journal = {Robotics and Autonomous Systems},
	author = {Morimoto, Jun and Doya, Kenji},
	month = jul,
	year = {2001},
	keywords = {Reinforcement learning, Hierarchical, Motor control, Real robot, Stand-up},
	pages = {37--51},
	file = {ScienceDirect Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/62MRSHDM/S0921889001001130.html:text/html}
}

@article{mehta_transfer_2008,
	title = {Transfer in variable-reward hierarchical reinforcement learning},
	volume = {73},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-008-5061-y},
	doi = {10.1007/s10994-008-5061-y},
	abstract = {Transfer learning seeks to leverage previously learned tasks to achieve faster learning in a new task. In this paper, we consider transfer learning in the context of related but distinct Reinforcement Learning (RL) problems. In particular, our RL problems are derived from Semi-Markov Decision Processes (SMDPs) that share the same transition dynamics but have different reward functions that are linear in a set of reward features. We formally define the transfer learning problem in the context of RL as learning an efficient algorithm to solve any SMDP drawn from a fixed distribution after experiencing a finite number of them. Furthermore, we introduce an online algorithm to solve this problem, Variable-Reward Reinforcement Learning (VRRL), that compactly stores the optimal value functions for several SMDPs, and uses them to optimally initialize the value function for a new SMDP. We generalize our method to a hierarchical RL setting where the different SMDPs share the same task hierarchy. Our experimental results in a simplified real-time strategy domain show that significant transfer learning occurs in both flat and hierarchical settings. Transfer is especially effective in the hierarchical setting where the overall value functions are decomposed into subtask value functions which are more widely amenable to transfer across different SMDPs.},
	language = {en},
	number = {3},
	urldate = {2019-01-01},
	journal = {Machine Learning},
	author = {Mehta, Neville and Natarajan, Sriraam and Tadepalli, Prasad and Fern, Alan},
	month = jun,
	year = {2008},
	keywords = {Average-reward learning, Hierarchical reinforcement learning, Multi-criteria learning, Transfer learning},
	pages = {289},
	file = {Springer Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/6YC9P9AJ/Mehta et al. - 2008 - Transfer in variable-reward hierarchical reinforce.pdf:application/pdf}
}

@article{botvinick_hierarchically_2009,
	series = {Reinforcement learning and higher cognition},
	title = {Hierarchically organized behavior and its neural foundations: {A} reinforcement learning perspective},
	volume = {113},
	issn = {0010-0277},
	shorttitle = {Hierarchically organized behavior and its neural foundations},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027708002059},
	doi = {10.1016/j.cognition.2008.08.011},
	abstract = {Research on human and animal behavior has long emphasized its hierarchical structure—the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
	number = {3},
	urldate = {2019-01-01},
	journal = {Cognition},
	author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andrew C.},
	month = dec,
	year = {2009},
	keywords = {Reinforcement learning, Prefrontal cortex},
	pages = {262--280},
	file = {Accepted Version:/Users/guillaume/Documents/Scolaire/Zotero/storage/CDLF6QTR/Botvinick et al. - 2009 - Hierarchically organized behavior and its neural f.pdf:application/pdf;ScienceDirect Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/TDJHMCWY/S0010027708002059.html:text/html}
}

@article{vezhnevets_feudal_2017,
	title = {{FeUdal} {Networks} for {Hierarchical} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1703.01161},
	abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
	urldate = {2019-01-01},
	journal = {arXiv:1703.01161 [cs]},
	author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.01161},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:1703.01161 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/J3WKSDZQ/Vezhnevets et al. - 2017 - FeUdal Networks for Hierarchical Reinforcement Lea.pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/62I2U3JX/1703.html:text/html}
}

@inproceedings{li_efficient_2017,
	title = {An {Efficient} {Approach} to {Model}-{Based} {Hierarchical} {Reinforcement} {Learning}},
	abstract = {We propose a model-based approach to hierarchical reinforcement learning that exploits shared knowledge and selective execution at different levels of abstraction, to efficiently solve large, complex problems. Our framework adopts a new transition dynamics learning algorithm that identifies the common action-feature combinations of the subtasks, and evaluates the subtask execution choices through simulation. The framework is sample efficient, and tolerates uncertain and incomplete problem characterization of the subtasks. We test the framework on common benchmark problems and complex simulated robotic environments. It compares favorably against the stateof-the-art algorithms, and scales well in very large problems.},
	booktitle = {{AAAI}},
	author = {Li, Zhuoru and Narayan, Akshay and Leong, Tze-Yun},
	year = {2017},
	keywords = {Reinforcement learning, Algorithm, Benchmark (computing), Principle of abstraction, Robot, Simulation},
	file = {Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/2GRF6FXG/Li et al. - 2017 - An Efficient Approach to Model-Based Hierarchical .pdf:application/pdf}
}

@article{botvinick_model-based_2014,
	title = {Model-based hierarchical reinforcement learning and human action control},
	volume = {369},
	issn = {0962-8436},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4186233/},
	doi = {10.1098/rstb.2013.0480},
	abstract = {Recent work has reawakened interest in goal-directed or ‘model-based’ choice, where decisions are based on prospective evaluation of potential action outcomes. Concurrently, there has been growing attention to the role of hierarchy in decision-making and action control. We focus here on the intersection between these two areas of interest, considering the topic of hierarchical model-based control. To characterize this form of action control, we draw on the computational framework of hierarchical reinforcement learning, using this to interpret recent empirical findings. The resulting picture reveals how hierarchical model-based mechanisms might play a special and pivotal role in human decision-making, dramatically extending the scope and complexity of human behaviour.},
	number = {1655},
	urldate = {2019-01-01},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Botvinick, Matthew and Weinstein, Ari},
	month = nov,
	year = {2014},
	pmid = {25267822},
	pmcid = {PMC4186233},
	file = {PubMed Central Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/LN5X9AAS/Botvinick and Weinstein - 2014 - Model-based hierarchical reinforcement learning an.pdf:application/pdf}
}

@inproceedings{mcgovern_automatic_2001,
	address = {San Francisco, CA, USA},
	series = {{ICML} '01},
	title = {Automatic {Discovery} of {Subgoals} in {Reinforcement} {Learning} {Using} {Diverse} {Density}},
	isbn = {978-1-55860-778-1},
	url = {http://dl.acm.org/citation.cfm?id=645530.655681},
	urldate = {2019-01-01},
	booktitle = {Proceedings of the {Eighteenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {McGovern, Amy and Barto, Andrew G.},
	year = {2001},
	pages = {361--368},
	file = {McGovern and Barto - 2001 - Automatic Discovery of Subgoals in Reinforcement L.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/KEXNXB77/McGovern and Barto - 2001 - Automatic Discovery of Subgoals in Reinforcement L.pdf:application/pdf}
}

@phdthesis{precup_temporal_2000,
	type = {{PhD} {Thesis}},
	title = {Temporal {Abstraction} in {Reinforcement} {Learning}},
	abstract = {Decision making usually involves choosing among different courses of action over a broad range of time scales. For instance, a person planning a trip to a distant location makes high-level decisions regarding what means of transportation to use, but also chooses low-level actions, such as the movements for getting into a car. The problem of picking an appropriate time scale for reasoning and learning has been explored in artificial intelligence, control theory and robotics. In this dissertation we develop a framework that allows novel solutions to this problem, in the context of Markov Decision Processes (MDPs) and reinforcement learning. In this dissertation, we present a general framework for prediction, control and learning at multiple temporal scales. In this framework, temporally extended actions are represented by a way of behaving (a policy) together with a termination condition. An action represented in this way is called an option. Options can be easily incorporated in MDPs, allowing an agent to use existing controllers, heuristics for picking actions, or learned courses of action. The effects of behaving according to an option can be predicted using multi-time models, learned by interacting with the environment. In this dissertation we develop multi-time models, and we illustrate the way in which they can be used to produce plans of behavior very quickly, using classical dynamic programming or reinforcement learning techniques. The most interesting feature of our framework is that it allows an agent to work simultaneously with high-level and low-level temporal representations. The interplay of these levels can be exploited in order to learn and plan more efficiently and more accurately. We develop new algorithms that take advantage of this structure to improve the quality of plans, and to learn in parallel about the effects of many different options.},
	school = {University of Massachusetts Amherst},
	author = {Precup, Doina},
	year = {2000},
	annote = {AAI9978540},
	file = {Citeseer - Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/9QVWG4AU/Precup - 2000 - Temporal Abstraction in Reinforcement Learning.pdf:application/pdf}
}

@article{bacon_option-critic_2016,
	title = {The {Option}-{Critic} {Architecture}},
	url = {http://arxiv.org/abs/1609.05140},
	abstract = {Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup \& Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.},
	urldate = {2019-01-01},
	journal = {arXiv:1609.05140 [cs]},
	author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.05140},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to the Thirthy-first AAAI Conference On Artificial Intelligence (AAAI), 2017},
	file = {arXiv\:1609.05140 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/T83A9CXU/Bacon et al. - 2016 - The Option-Critic Architecture.pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/RJ58IPFZ/1609.html:text/html}
}

@inproceedings{dayan_feudal_1993,
	address = {San Francisco, CA, USA},
	title = {Feudal {Reinforcement} {Learning}},
	isbn = {978-1-55860-274-8},
	url = {http://dl.acm.org/citation.cfm?id=645753.668239},
	urldate = {2019-01-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 5, [{NIPS} {Conference}]},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Dayan, Peter and Hinton, Geoffrey E.},
	year = {1993},
	pages = {271--278},
	file = {Dayan - Feudal Reinforcement Learning.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/ZA6TKTBL/Dayan - Feudal Reinforcement Learning.pdf:application/pdf}
}

@article{kaelbling_reinforcement_1996,
	title = {Reinforcement {Learning}: {A} {Survey}},
	volume = {4},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {Reinforcement {Learning}},
	url = {https://www.jair.org/index.php/jair/article/view/10166},
	doi = {10.1613/jair.301},
	language = {en},
	urldate = {2019-01-06},
	journal = {Journal of Artificial Intelligence Research},
	author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
	month = may,
	year = {1996},
	pages = {237--285},
	file = {Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/KPZN7DXB/Kaelbling et al. - 1996 - Reinforcement Learning A Survey.pdf:application/pdf;Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/E32INMPL/10166.html:text/html}
}

@incollection{carbonell_theoretical_1998,
	address = {Berlin, Heidelberg},
	title = {Theoretical results on reinforcement learning with temporally abstract options},
	volume = {1398},
	isbn = {978-3-540-64417-0 978-3-540-69781-7},
	url = {http://link.springer.com/10.1007/BFb0026709},
	abstract = {We present new theoretical results on planning within the framework of temporally abstract reinforcement learning (Precup \& Sutton, 1997; Sutton, 1995). Temporal abstraction is a key step in any decision making system that involves planning and prediction. In temporally abstract reinforcement learning, the agent is allowed to choose among ”options”, whole courses of action that may be temporally extended, stochastic, and contingent on previous events. Examples of options include closed-loop policies such as picking up an object, as well as primitive actions such as joint torques. Knowledge about the consequences of options is represented by special structures called multi-time models. In this paper we focus on the theory of planning with multi-time models. We deﬁne new Bellman equations that are satisﬁed for sets of multi-time models. As a consequence, multi-time models can be used interchangeably with models of primitive actions in a variety of well-known planning methods including value iteration, policy improvement and policy iteration.},
	language = {en},
	urldate = {2019-01-06},
	booktitle = {Machine {Learning}: {ECML}-98},
	publisher = {Springer Berlin Heidelberg},
	author = {Precup, Doina and Sutton, Richard S. and Singh, Satinder},
	editor = {Carbonell, Jaime G. and Siekmann, Jörg and Goos, G. and Hartmanis, J. and van Leeuwen, J. and Nédellec, Claire and Rouveirol, Céline},
	year = {1998},
	doi = {10.1007/BFb0026709},
	pages = {382--393},
	file = {Precup et al. - 1998 - Theoretical results on reinforcement learning with.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/63FS9DCM/Precup et al. - 1998 - Theoretical results on reinforcement learning with.pdf:application/pdf}
}

@inproceedings{simsek_identifying_2005,
	address = {Bonn, Germany},
	title = {Identifying useful subgoals in reinforcement learning by local graph partitioning},
	isbn = {978-1-59593-180-1},
	url = {http://portal.acm.org/citation.cfm?doid=1102351.1102454},
	doi = {10.1145/1102351.1102454},
	abstract = {We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identiﬁes subgoals by partitioning local state transition graphs—those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek—states that lie between two densely-connected regions of the state space—while producing an algorithm with low computational cost.},
	language = {en},
	urldate = {2019-01-06},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning  - {ICML} '05},
	publisher = {ACM Press},
	author = {Şimşek, Özgür and Wolfe, Alicia P. and Barto, Andrew G.},
	year = {2005},
	pages = {816--823},
	file = {Şimşek et al. - 2005 - Identifying useful subgoals in reinforcement learn.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/93XIB2JY/Şimşek et al. - 2005 - Identifying useful subgoals in reinforcement learn.pdf:application/pdf}
}

@incollection{precup_multi-time_1998,
	title = {Multi-time {Models} for {Temporally} {Abstract} {Planning}},
	url = {http://papers.nips.cc/paper/1362-multi-time-models-for-temporally-abstract-planning.pdf},
	urldate = {2019-01-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 10},
	publisher = {MIT Press},
	author = {Precup, Doina and Sutton, Richard S},
	editor = {Jordan, M. I. and Kearns, M. J. and Solla, S. A.},
	year = {1998},
	pages = {1050--1056},
	file = {NIPS Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/4LRQ222X/Precup and Sutton - 1998 - Multi-time Models for Temporally Abstract Planning.pdf:application/pdf;NIPS Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/P8NI5S5H/1362-multi-time-models-for-temporally-abstract-planning.html:text/html}
}

@inproceedings{ravindran_model_2002,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Model {Minimization} in {Hierarchical} {Reinforcement} {Learning}},
	isbn = {978-3-540-45622-3},
	abstract = {When applied to real world problems Markov Decision Processes (MDPs) often exhibit considerable implicit redundancy, especially when there are symmetries in the problem. In this article we present an MDP minimization framework based on homomorphisms. The framework exploits redundancy and symmetry to derive smaller equivalent models of the problem. We then apply our minimization ideas to the options framework to derive relativized options—options defined without an absolute frame of reference. We demonstrate their utility empirically even in cases where the minimization criteria are not met exactly.},
	language = {en},
	booktitle = {Abstraction, {Reformulation}, and {Approximation}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ravindran, Balaraman and Barto, Andrew G.},
	editor = {Koenig, Sven and Holte, Robert C.},
	year = {2002},
	pages = {196--211},
	file = {ravindran_b_SARA02.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/8GBSWWPH/ravindran_b_SARA02.pdf:application/pdf}
}

@inproceedings{jong_utility_2008,
	title = {The utility of temporal abstraction in reinforcement learning},
	abstract = {The hierarchical structure of real-world problems has motivated extensive research into temporal abstractions for reinforcement learning, but precisely how these abstractions allow agents to improve their learning performance is not well understood. This paper investigates the connection between temporal abstraction and an agent’s exploration policy, which determines how the agent’s performance improves over time. Experimental results with standard methods for incorporating temporal abstractions show that these methods benefit learning only in limited contexts. The primary contribution of this paper is a clearer understanding of how hierarchical decompositions interact with reinforcement learning algorithms, with important consequences for the manual design or automatic discovery of action hierarchies.},
	booktitle = {Proceedings of the {Seventh} {International} {Joint} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}},
	author = {Jong, Nicholas K.},
	year = {2008},
	file = {Citeseer - Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/39ANYJKI/Jong - 2008 - The utility of temporal abstraction in reinforceme.pdf:application/pdf;Citeseer - Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/DAGM2MDC/summary.html:text/html}
}

@inproceedings{singh_scaling_1992,
	title = {Scaling {Reinforcement} {Learning} {Algorithms} by {Learning} {Variable} {Temporal} {Resolution} {Models}},
	abstract = {The close connection between reinforcement  learning (RL) algorithms and dynamic programming  algorithms has fueled research on  RL within the machine learning community.  Yet, despite increased theoretical understanding,  RL algorithms remain applicable  to simple tasks only. In this paper  I use the abstract framework afforded by  the connection to dynamic programming to  discuss the scaling issues faced by RL researchers.  I focus on learning agents that  have to learn to solve multiple structured RL  tasks in the same environment. I propose  learning abstract environment models where  the abstract actions represent "intentions" of  achieving a particular state. Such models are  variable temporal resolution models because  in different parts of the state space the abstract  actions span different number of time  steps. The operational definitions of abstract  actions can be learned incrementally using  repeated experience at solving RL tasks. I  prove that under certain conditions s...},
	booktitle = {In {Proceedings} of the {Ninth} {International} {Machine} {Learning} {Conference}},
	publisher = {Morgan Kaufmann},
	author = {Singh, Satinder P.},
	year = {1992},
	pages = {406--415},
	file = {Citeseer - Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/AZVYSDRH/Singh - 1992 - Scaling Reinforcement Learning Algorithms by Learn.pdf:application/pdf;Citeseer - Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/T8LVC4CB/summary.html:text/html}
}

@incollection{hernandez-gardiol_hierarchical_2001,
	title = {Hierarchical {Memory}-{Based} {Reinforcement} {Learning}},
	url = {http://papers.nips.cc/paper/1837-hierarchical-memory-based-reinforcement-learning.pdf},
	urldate = {2019-01-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 13},
	publisher = {MIT Press},
	author = {Hernandez-Gardiol, Natalia and Mahadevan, Sridhar},
	editor = {Leen, T. K. and Dietterich, T. G. and Tresp, V.},
	year = {2001},
	pages = {1047--1053},
	file = {NIPS Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/WSZVFKW9/Hernandez-Gardiol and Mahadevan - 2001 - Hierarchical Memory-Based Reinforcement Learning.pdf:application/pdf;NIPS Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/T5Q57ZHU/1837-hierarchical-memory-based-reinforcement-learning.html:text/html}
}

@incollection{goos_q-cutdynamic_2002,
	address = {Berlin, Heidelberg},
	title = {Q-{Cut}—{Dynamic} {Discovery} of {Sub}-goals in {Reinforcement} {Learning}},
	volume = {2430},
	isbn = {978-3-540-44036-9 978-3-540-36755-0},
	url = {http://link.springer.com/10.1007/3-540-36755-1_25},
	abstract = {We present the Q-Cut algorithm, a graph theoretic approach for automatic detection of sub-goals in a dynamic environment, which is used for acceleration of the Q-Learning algorithm. The learning agent creates an on-line map of the process history, and uses an eﬃcient MaxFlow/Min-Cut algorithm for identifying bottlenecks. The policies for reaching bottlenecks are separately learned and added to the model in a form of options (macro-actions). We then extend the basic Q-Cut algorithm to the Segmented Q-Cut algorithm, which uses previously identiﬁed bottlenecks for state space partitioning, necessary for ﬁnding additional bottlenecks in complex environments. Experiments show signiﬁcant performance improvements, particulary in the initial learning phase.},
	language = {en},
	urldate = {2019-01-06},
	booktitle = {Machine {Learning}: {ECML} 2002},
	publisher = {Springer Berlin Heidelberg},
	author = {Menache, Ishai and Mannor, Shie and Shimkin, Nahum},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Elomaa, Tapio and Mannila, Heikki and Toivonen, Hannu},
	year = {2002},
	doi = {10.1007/3-540-36755-1_25},
	pages = {295--306},
	file = {Menache et al. - 2002 - Q-Cut—Dynamic Discovery of Sub-goals in Reinforcem.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/RZAYKV4M/Menache et al. - 2002 - Q-Cut—Dynamic Discovery of Sub-goals in Reinforcem.pdf:application/pdf}
}

@inproceedings{morimoto_hierarchical_1998,
	title = {Hierarchical {Reinforcement} {Learning} of {Low}-{Dimensional} {Subgoals} and {High}-{Dimensional} {Trajectories}},
	abstract = {In this paper, we propose a hierarchical reinforcement learning method which enables a learner to learn tasks in a highdimensional state space. In the upper level, the learner coarsely explores the low-dimensional state space. In the lower level, the learner finely explores the high-dimensional state space. Specifically, the learner learns to set up appropriate subgoals for the task in the upper level, and learns to achieve the subgoals in the lower level. As an example task, we choose a stand-up task involving a two-joint three-link robot. This robot has a ten-dimensional state space. The robot learns to find subgoal postures in the upper level, and to achieve these subgoal postures in the lower level. Simulation results show that the hierarchical architecture acceralates the learning of the robot to stand up.  KEYWORDS: Reinforcement learning, Hierarchical, Stand up, Robotics  1. Introduction  Reinforcement learning does not require explicit knowledge on the desired trajectories, onl...},
	booktitle = {in {Proceedings} of the 5th {International} {Conference} on {Neural} {Information} {Processing}},
	author = {Morimoto, Jun and Doya, Kenji},
	year = {1998},
	pages = {850--853},
	file = {Citeseer - Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/FPFULGME/Morimoto and Doya - 1998 - Hierarchical Reinforcement Learning of Low-Dimensi.pdf:application/pdf;Citeseer - Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/9QI98EC5/summary.html:text/html}
}

@article{nason_soar-rl:_2005,
	title = {Soar-{RL}: integrating reinforcement learning with {Soar}},
	volume = {6},
	issn = {13890417},
	shorttitle = {Soar-{RL}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1389041704000646},
	doi = {10.1016/j.cogsys.2004.09.006},
	abstract = {In this paper, we describe an architectural modiﬁcation to Soar that gives a Soar agent the opportunity to learn statistical information about the past success of its actions and utilize this information when selecting an operator. This mechanism serves the same purpose as production utilities in ACT-R, but the implementation is more directly tied to the standard deﬁnition of the reinforcement learning (RL) problem. The paper explains our implementation, gives a rationale for adding an RL capability to Soar, and shows results for Soar-RL agentsÕ performance on two tasks.},
	language = {en},
	number = {1},
	urldate = {2019-01-06},
	journal = {Cognitive Systems Research},
	author = {Nason, Shelley and Laird, John E.},
	month = mar,
	year = {2005},
	pages = {51--59},
	file = {Nason and Laird - 2005 - Soar-RL integrating reinforcement learning with S.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/N7S65M37/Nason and Laird - 2005 - Soar-RL integrating reinforcement learning with S.pdf:application/pdf}
}

@inproceedings{comanici_optimal_2010,
	address = {Richland, SC},
	series = {{AAMAS} '10},
	title = {Optimal {Policy} {Switching} {Algorithms} for {Reinforcement} {Learning}},
	isbn = {978-0-9826571-1-9},
	url = {http://dl.acm.org/citation.cfm?id=1838206.1838300},
	abstract = {We address the problem of single-agent, autonomous sequential decision making. We assume that some controllers or behavior policies are given as prior knowledge, and the task of the agent is to learn how to switch between these policies. We formulate the problem using the framework of reinforcement learning and options (Sutton, Precup \& Singh, 1999; Precup, 2000). We derive gradient-based algorithms for learning the termination conditions of options, with the goal of optimizing the expected long-term return. We incorporate the proposed approach into policy-gradient methods with linear function approximation.},
	urldate = {2019-01-07},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}: {Volume} 1 - {Volume} 1},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Comanici, Gheorghe and Precup, Doina},
	year = {2010},
	keywords = {Markov decision processes, reinforcement learning, policy gradient, temporal abstraction},
	pages = {709--714},
	file = {Comanici and Precup - Optimal Policy Switching Algorithms for Reinforcem.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/A6JYDP5L/Comanici and Precup - Optimal Policy Switching Algorithms for Reinforcem.pdf:application/pdf}
}

@inproceedings{dietterich_maxq_1998,
	address = {San Francisco, CA, USA},
	series = {{ICML} '98},
	title = {The {MAXQ} {Method} for {Hierarchical} {Reinforcement} {Learning}},
	isbn = {978-1-55860-556-5},
	url = {http://dl.acm.org/citation.cfm?id=645527.657449},
	urldate = {2019-01-07},
	booktitle = {Proceedings of the {Fifteenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Dietterich, Thomas G.},
	year = {1998},
	pages = {118--126},
	file = {Dietterich - The MAXQ Method for Hierarchical Reinforcement Lea.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/9VGPN7QN/Dietterich - The MAXQ Method for Hierarchical Reinforcement Lea.pdf:application/pdf}
}

@inproceedings{goel_subgoal_2003,
	title = {Subgoal {Discovery} for {Hierarchical} {Reinforcement} {Learning} {Using} {Learned} {Policies}.},
	abstract = {Reinforcement learning addresses the problem of learning to select actions in order to maximize an agent's performance in unknown environments. To scale reinforcement learning to complex real-world tasks, agent must be able to discover hierarchical structures within their learning and control systems. This paper presents a method by which a reinforcement learning agent can discover subgoals with certain structural properties. By discovering subgoals and including policies to subgoals as actions in its action set, the agent is able to explore more effectively and accelerate learning in other tasks in the same or similar environments where the same subgoals are useful. The agent discovers the subgoals by searching a learned policy model for state that exhibits certain structural properties. This approach is illustrated using gridworld tasks.},
	author = {Goel, Sandeep and Huber, Manfred},
	month = jan,
	year = {2003},
	pages = {346--350},
	file = {Goel and Huber - Subgoal Discovery for Hierarchical Reinforcement L.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/HQRG7IQ6/Goel and Huber - Subgoal Discovery for Hierarchical Reinforcement L.pdf:application/pdf}
}

@article{haarnoja_latent_2018,
	title = {Latent {Space} {Policies} for {Hierarchical} {Reinforcement} {Learning}},
	abstract = {We address the problem of learning hierarchical deep neural network policies for reinforcement learning. Our aim is to design a hierarchical reinforcement learning algorithm that can construct hierarchical representations in bottom-up layerwise fashion. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer. The maximum entropy objective causes these latent variables to be incorporated into the layer's policy, and the higher level layer can directly control the behavior of the lower layer through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.},
	author = {Haarnoja, Tuomas and Hartikainen, Kristian and Abbeel, Pieter and Levine, Sergey},
	month = apr,
	year = {2018},
	file = {Haarnoja et al. - Latent Space Policies for Hierarchical Reinforceme.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/V4WMTTF6/Haarnoja et al. - Latent Space Policies for Hierarchical Reinforceme.pdf:application/pdf}
}

@inproceedings{jong_hierarchical_2008,
	title = {Hierarchical model-based reinforcement learning: {R}-max + {MAXQ}},
	shorttitle = {Hierarchical model-based reinforcement learning},
	doi = {10.1145/1390156.1390211},
	abstract = {Hierarchical decomposition promises to help scale reinforcement learning algorithms naturally to real-world problems by exploiting their underlying structure. Model-based algorithms, which provided the first finite-time convergence guarantees for reinforcement learning, may also play an important role in coping with the relative scarcity of data in large environments. In this paper, we introduce an algorithm that fully integrates modern hierarchical and model-learning methods in the standard reinforcement learning setting. Our algorithm, R-maxq, inherits the efficient model-based exploration of the R-max algorithm and the opportunities for abstraction provided by the MAXQ framework. We analyze the sample complexity of our algorithm, and our experiments in a standard simulation environment illustrate the advantages of combining hierarchies and models.},
	booktitle = {{ICML} '08},
	author = {Jong, Nicholas K. and Stone, Peter},
	year = {2008},
	keywords = {Machine learning, Reinforcement learning, Algorithm, Simulation, Coping Behavior, Experiment, Sample complexity},
	file = {Jong and Stone - Hierarchical Model-Based Reinforcement Learning R.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/TDAWRI2L/Jong and Stone - Hierarchical Model-Based Reinforcement Learning R.pdf:application/pdf}
}

@article{konidaris_building_2006,
	title = {Building {Portable} {Options}: {Skill} {Transfer} in {Reinforcement} {Learning}},
	shorttitle = {Building {Portable} {Options}},
	abstract = {The options framework provides methods for reinforcement learning agents to build new high-level skills. However, since options are usually learned in the same state space as the problem the agent is solving, they cannot be used in other tasks that are similar but have different state spaces. We introduce the notion of learning options in agentspace, the space generated by a feature set that is present and retains the same semantics across successive problem instances, rather than in problemspace. Agent-space options can be reused in later tasks that share the same agent-space but have different problem-spaces. We present experimental results demonstrating the use of agent-space options in building transferrable skills, and show that they perform best when used in conjunction with problem-space options.},
	journal = {Learning},
	author = {Konidaris, George and G. Barto, Andrew},
	month = jan,
	year = {2006},
	pages = {1--13},
	file = {Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/XB5IGAEY/Konidaris and G. Barto - 2006 - Building Portable Options Skill Transfer in Reinf.pdf:application/pdf}
}

@misc{machado_laplacian_2017,
	title = {A {Laplacian} {Framework} for {Option} {Discovery} in {Reinforcement} {Learning}},
	url = {/paper/A-Laplacian-Framework-for-Option-Discovery-in-Machado-Bellemare/8423cc50c18d68f797adaa4f571f5e4efbe325a5},
	abstract = {Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-RL is a well known approach for representation learning in MDPs. The representations learned with this framework are called proto-value functions (PVFs). In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are independent of the agents’ intentions. Moreover, by capturing the diffusion process of a random walk, different options act at different time scales, making them helpful for exploration strategies. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.},
	language = {en},
	urldate = {2019-01-07},
	journal = {undefined},
	author = {Machado, Marlos C. and Bellemare, Marc G. and Bowling, Michael H.},
	year = {2017},
	file = {Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/GLNKFILE/Machado et al. - 2017 - A Laplacian Framework for Option Discovery in Rein.pdf:application/pdf;Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/6V3C4WWX/8423cc50c18d68f797adaa4f571f5e4efbe325a5.html:text/html}
}

@inproceedings{marthi_concurrent_2005,
	title = {Concurrent {Hierarchical} {Reinforcement} {Learning}.},
	abstract = {We describe a language for partially specifying policies in do- mains consisting of multiple subagents working together to maximize a common reward function. The language extends ALisp with constructs for concurrency and dynamic assign- ment of subagents to tasks. During learning, the subagents learn a distributed representation of the Q-function for this partial policy. They then coordinate at runtime to find the best joint action at each step. We give examples showing that programs in this language are natural and concise. We also describe online and batch learning algorithms for learning a linear approximation to the Q-function, which make use of the coordination structure of the problem.},
	author = {Marthi, Bhaskara and J. Russell, Stuart and Latham, David and Guestrin, Carlos},
	month = jan,
	year = {2005},
	pages = {779--785},
	file = {Marthi et al. - Concurrent Hierarchical Reinforcement Learning.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/UN4KBBRT/Marthi et al. - Concurrent Hierarchical Reinforcement Learning.pdf:application/pdf}
}

@article{florensa_stochastic_2017,
	title = {Stochastic {Neural} {Networks} for {Hierarchical} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1704.03012},
	abstract = {Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.},
	urldate = {2019-01-07},
	journal = {arXiv:1704.03012 [cs]},
	author = {Florensa, Carlos and Duan, Yan and Abbeel, Pieter},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.03012},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	file = {arXiv\:1704.03012 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/IBKSZKMA/Florensa et al. - 2017 - Stochastic Neural Networks for Hierarchical Reinfo.pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/M8TKQNE2/1704.html:text/html}
}

@article{frans_meta_2017,
	title = {Meta {Learning} {Shared} {Hierarchies}},
	url = {http://arxiv.org/abs/1710.09767},
	abstract = {We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.},
	urldate = {2019-01-07},
	journal = {arXiv:1710.09767 [cs]},
	author = {Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.09767},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1710.09767 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/MXIDMCY9/Frans et al. - 2017 - Meta Learning Shared Hierarchies.pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/P9IB7ZKJ/1710.html:text/html}
}

@article{peng_deeploco:_2017,
	title = {{DeepLoco}: dynamic locomotion skills using hierarchical deep reinforcement learning},
	volume = {36},
	issn = {07300301},
	shorttitle = {{DeepLoco}},
	url = {http://dl.acm.org/citation.cfm?doid=3072959.3073602},
	doi = {10.1145/3072959.3073602},
	language = {en},
	number = {4},
	urldate = {2019-01-07},
	journal = {ACM Transactions on Graphics},
	author = {Peng, Xue Bin and Berseth, Glen and Yin, Kangkang and Van De Panne, Michiel},
	month = jul,
	year = {2017},
	pages = {1--13},
	file = {Peng et al. - 2017 - DeepLoco dynamic locomotion skills using hierarch.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/RT9P8D7P/Peng et al. - 2017 - DeepLoco dynamic locomotion skills using hierarch.pdf:application/pdf}
}

@article{alexander_strategic_2016,
	title = {Strategic {Attentive} {Writer} for {Learning} {Macro}-{Actions}},
	url = {http://arxiv.org/abs/1606.04695},
	abstract = {We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.},
	urldate = {2019-01-07},
	journal = {arXiv:1606.04695 [cs]},
	author = {Alexander and Vezhnevets and Mnih, Volodymyr and Agapiou, John and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04695},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1606.04695 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/J48TS5XR/Alexander et al. - 2016 - Strategic Attentive Writer for Learning Macro-Acti.pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/RKP5BDAN/1606.html:text/html}
}

@article{ghazanfari_autonomous_2017,
	title = {Autonomous {Extracting} a {Hierarchical} {Structure} of {Tasks} in {Reinforcement} {Learning} and {Multi}-task {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1709.04579},
	abstract = {Reinforcement learning (RL), while often powerful, can suffer from slow learning speeds, particularly in high dimensional spaces. The autonomous decomposition of tasks and use of hierarchical methods hold the potential to significantly speed up learning in such domains. This paper proposes a novel practical method that can autonomously decompose tasks, by leveraging association rule mining, which discovers hidden relationship among entities in data mining. We introduce a novel method called ARM-HSTRL (Association Rule Mining to extract Hierarchical Structure of Tasks in Reinforcement Learning). It extracts temporal and structural relationships of sub-goals in RL, and multi-task RL. In particular,it finds sub-goals and relationship among them. It is shown the significant efficiency and performance of the proposed method in two main topics of RL.},
	urldate = {2019-01-07},
	journal = {arXiv:1709.04579 [cs]},
	author = {Ghazanfari, Behzad and Taylor, Matthew E.},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.04579},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:1709.04579 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/EHFRRU77/Ghazanfari and Taylor - 2017 - Autonomous Extracting a Hierarchical Structure of .pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/RP6VW33K/1709.html:text/html}
}

@inproceedings{digney_learning_1998,
	address = {Cambridge, MA, USA},
	title = {Learning {Hierarchical} {Control} {Structures} for {Multiple} {Tasks} and {Changing} {Environments}},
	isbn = {978-0-262-66144-7},
	url = {http://dl.acm.org/citation.cfm?id=299955.299998},
	urldate = {2019-01-07},
	booktitle = {Proceedings of the {Fifth} {International} {Conference} on {Simulation} of {Adaptive} {Behavior} on {From} {Animals} to {Animats} 5},
	publisher = {MIT Press},
	author = {Digney, Bruce L.},
	year = {1998},
	pages = {321--330},
	file = {Digney - Learning HierarchicaClhCaonngtirnogl EStnrvuircotu.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/Z78HLFL7/Digney - Learning HierarchicaClhCaonngtirnogl EStnrvuircotu.pdf:application/pdf}
}

@article{jonsson_causal_2006,
	title = {Causal {Graph} {Based} {Decomposition} of {Factored} {MDPs}},
	volume = {7},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v7/jonsson06a.html},
	number = {Nov},
	urldate = {2019-01-07},
	journal = {Journal of Machine Learning Research},
	author = {Jonsson, Anders and Barto, Andrew},
	year = {2006},
	pages = {2259--2301},
	file = {Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/H5BABSNW/Jonsson and Barto - 2006 - Causal Graph Based Decomposition of Factored MDPs.pdf:application/pdf;Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/9PQL8DLA/jonsson06a.html:text/html}
}

@inproceedings{mehta_automatic_2008,
	address = {New York, NY, USA},
	series = {{ICML} '08},
	title = {Automatic {Discovery} and {Transfer} of {MAXQ} {Hierarchies}},
	isbn = {978-1-60558-205-4},
	url = {http://doi.acm.org/10.1145/1390156.1390238},
	doi = {10.1145/1390156.1390238},
	abstract = {We present an algorithm, HI-MAT (Hierarchy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by applying dynamic Bayesian network models to a successful trajectory from a source reinforcement learning task. HI-MAT discovers subtasks by analyzing the causal and temporal relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consistent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empirically that HI-MAT constructs compact hierarchies that are comparable to manually-engineered hierarchies and facilitate significant speedup in learning when transferred to a target task.},
	urldate = {2019-01-07},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Mehta, Neville and Ray, Soumya and Tadepalli, Prasad and Dietterich, Thomas},
	year = {2008},
	pages = {648--655},
	file = {Submitted Version:/Users/guillaume/Documents/Scolaire/Zotero/storage/UDMU355Q/Mehta et al. - 2008 - Automatic Discovery and Transfer of MAXQ Hierarchi.pdf:application/pdf}
}

@inproceedings{mannor_dynamic_2004,
	address = {New York, NY, USA},
	series = {{ICML} '04},
	title = {Dynamic {Abstraction} in {Reinforcement} {Learning} via {Clustering}},
	isbn = {978-1-58113-838-2},
	url = {http://doi.acm.org/10.1145/1015330.1015355},
	doi = {10.1145/1015330.1015355},
	abstract = {We consider a graph theoretic approach for automatic construction of options in a dynamic environment. A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions. A clustering algorithm is then used to partition the state space to different regions. Policies for reaching the different parts of the space are separately learned and added to the model in a form of options (macro-actions). The options are used for accelerating the Q-Learning algorithm. We extend the basic algorithm and consider building a map that includes preliminary indication of the location of "interesting" regions of the state space, where the value gradient is significant and additional exploration might be beneficial. Experiments indicate significant speedups, especially in the initial learning phase.},
	urldate = {2019-01-07},
	booktitle = {Proceedings of the {Twenty}-first {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Mannor, Shie and Menache, Ishai and Hoze, Amit and Klein, Uri},
	year = {2004},
	keywords = {Reinforcement Learning, Options, Clustering, Hierarchical Reinforcement Learning, Q-Learning},
	pages = {71--},
	file = {Submitted Version:/Users/guillaume/Documents/Scolaire/Zotero/storage/77G3RLX9/Mannor et al. - 2004 - Dynamic Abstraction in Reinforcement Learning via .pdf:application/pdf}
}

@article{schmidhuber_formal_2010,
	title = {Formal {Theory} of {Creativity}, {Fun}, and {Intrinsic} {Motivation} (1990–2010)},
	volume = {2},
	issn = {1943-0604, 1943-0612},
	url = {http://ieeexplore.ieee.org/document/5508364/},
	doi = {10.1109/TAMD.2010.2056368},
	abstract = {The simple but general formal theory of fun \& intrinsic motivation \& creativity (1990-) is based on the concept of maximizing intrinsic reward for the active creation or discovery of novel, surprising patterns allowing for improved prediction or data compression. It generalizes the traditional ﬁeld of active learning, and is related to old but less formal ideas in aesthetics theory and developmental psychology. It has been argued that the theory explains many essential aspects of intelligence including autonomous development, science, art, music, humor. This overview ﬁrst describes theoretically optimal (but not necessarily practical) ways of implementing the basic computational principles on exploratory, intrinsically motivated agents or robots, encouraging them to provoke event sequences exhibiting previously unknown but learnable algorithmic regularities. Emphasis is put on the importance of limited computational resources for online prediction and compression. Discrete and continuous time formulations are given. Previous practical but non-optimal implementations (1991, 1995, 1997-2002) are reviewed, as well as several recent variants by others (2005-). A simpliﬁed typology addresses current confusion concerning the precise nature of intrinsic motivation.},
	language = {en},
	number = {3},
	urldate = {2019-01-07},
	journal = {IEEE Transactions on Autonomous Mental Development},
	author = {Schmidhuber, Jürgen},
	month = sep,
	year = {2010},
	pages = {230--247},
	file = {Schmidhuber - 2010 - Formal Theory of Creativity, Fun, and Intrinsic Mo.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/CYALZHYP/Schmidhuber - 2010 - Formal Theory of Creativity, Fun, and Intrinsic Mo.pdf:application/pdf}
}

@inproceedings{stolle_learning_2002,
	title = {Learning {Options} in {Reinforcement} {Learning}},
	abstract = {Temporally extended actions (e.g., macro actions) have proven very  useful in speeding up learning, ensuring robustness and building prior knowledge  into AI systems. The options framework (Precup, 2000; Sutton, Precup \& Singh,  1999) provides a natural way of incorporating such actions into reinforcement  learning systems, but leaves open the issue of how good options might be identified.},
	booktitle = {Lecture {Notes} in {Computer} {Science}},
	author = {Stolle, Martin and Precup, Doina},
	year = {2002},
	pages = {212--223},
	file = {Citeseer - Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/53S2K6XI/Stolle and Precup - 2002 - Learning Options in Reinforcement Learning.pdf:application/pdf;Citeseer - Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/TII78HIK/summary.html:text/html}
}

@article{wiering_hq-learning_1998,
	title = {{HQ}-learning},
	volume = {6},
	issn = {1059-7123},
	url = {https://doi.org/10.1177/105971239700600202},
	doi = {10.1177/105971239700600202},
	number = {2},
	urldate = {2019-01-07},
	journal = {Adapt Behav},
	author = {Wiering, Marco and Schmidhuber, Jürgen},
	month = mar,
	year = {1998},
	keywords = {reinforcement learning, hierarchical Q-learning, non-Markov, POMDPs, subgoal learning},
	pages = {219--246},
	file = {Schmidhuber - Marco Wiering marco@idsia.ch.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/7GPD5RAK/Schmidhuber - Marco Wiering marco@idsia.ch.pdf:application/pdf}
}

@inproceedings{kaelbling_hierarchical_1993,
	title = {Hierarchical {Learning} in {Stochastic} {Domains}: {Preliminary} {Results}},
	shorttitle = {Hierarchical {Learning} in {Stochastic} {Domains}},
	abstract = {This paper presents the HDG learning algorithm,  which uses a hierarchical decomposition of the  state space to make learning to achieve goals  more efficient with a small penalty in path quality.  Special care must be taken when performing  hierarchical planning and learning in stochastic  domains, because macro-operators cannot be executed  ballistically. The HDG algorithm, which  is a descendent of Watkins' Q-learning algorithm,  is described here and preliminary empirical results  are presented.  1 INTRODUCTION  Reinforcement learning is a general tool for deriving strategies that optimize a fixed reinforcement function in a stochastic environment. A crucial problem in reinforcement learning is temporal credit assignment: how to choose actions based on good results that happen after (perhaps long after) the action is taken. This problem is solved well in the general case by temporal difference methods, such as Watkins' Q learning [Barto et al., 1989, Watkins, 1989] and Sutton's TD ...},
	booktitle = {In {Proceedings} of the {Tenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann},
	author = {Kaelbling, Leslie Pack},
	year = {1993},
	pages = {167--173},
	file = {Citeseer - Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/K89FKLIA/Kaelbling - 1993 - Hierarchical Learning in Stochastic Domains Preli.pdf:application/pdf;Citeseer - Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/P83KTA46/summary.html:text/html}
}

@article{brafman_r-max_2002,
	title = {R-{MAX} - {A} {General} {Polynomial} {Time} {Algorithm} for {Near}-{Optimal} {Reinforcement} {Learning}},
	volume = {3},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v3/brafman02a.html},
	number = {Oct},
	urldate = {2019-01-07},
	journal = {Journal of Machine Learning Research},
	author = {Brafman, Ronen I. and Tennenholtz, Moshe},
	year = {2002},
	pages = {213--231},
	file = {Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/GAEWUY7R/Brafman and Tennenholtz - 2002 - R-MAX - A General Polynomial Time Algorithm for Ne.pdf:application/pdf;Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/VIVHAREV/brafman02a.html:text/html}
}

@article{silver_compositional_2012,
	title = {Compositional {Planning} {Using} {Optimal} {Option} {Models}},
	url = {http://arxiv.org/abs/1206.6473},
	abstract = {In this paper we introduce a framework for option model composition. Option models are temporal abstractions that, like macro-operators in classical planning, jump directly from a start state to an end state. Prior work has focused on constructing option models from primitive actions, by intra-option model learning; or on using option models to construct a value function, by inter-option planning. We present a unified view of intra- and inter-option model learning, based on a major generalisation of the Bellman equation. Our fundamental operation is the recursive composition of option models into other option models. This key idea enables compositional planning over many levels of abstraction. We illustrate our framework using a dynamic programming algorithm that simultaneously constructs optimal option models for multiple subgoals, and also searches over those option models to provide rapid progress towards other subgoals.},
	urldate = {2019-01-07},
	journal = {arXiv:1206.6473 [cs]},
	author = {Silver, David and Ciosek, Kamil},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.6473},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)},
	file = {arXiv\:1206.6473 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/CBZP835A/Silver and Ciosek - 2012 - Compositional Planning Using Optimal Option Models.pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/CG7TRK5B/1206.html:text/html}
}

@article{guestrin_efficient_2003,
	title = {Efficient {Solution} {Algorithms} for {Factored} {MDPs}},
	volume = {19},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/10341},
	doi = {10.1613/jair.1000},
	abstract = {This paper addresses the problem of planning under uncertainty in large Markov Decision Processes (MDPs). Factored MDPs represent a complex state space using state variables and the transition model using a dynamic Bayesian network. This representation often allows an exponential reduction in the representation size of structured MDPs, but the complexity of exact solution algorithms for such MDPs can grow exponentially in the representation size. In this paper, we present two approximate solution algorithms that exploit structure in factored MDPs. Both use an approximate value function represented as a linear combination of basis functions, where each basis function involves only a small subset of the domain variables. A key contribution of this paper is that it shows how the basic operations of both algorithms can be performed eﬃciently in closed form, by exploiting both additive and context-speciﬁc structure in a factored MDP. A central element of our algorithms is a novel linear program decomposition technique, analogous to variable elimination in Bayesian networks, which reduces an exponentially large LP to a provably equivalent, polynomial-sized one. One algorithm uses approximate linear programming, and the second approximate dynamic programming. Our dynamic programming algorithm is novel in that it uses an approximation based on max-norm, a technique that more directly minimizes the terms that appear in error bounds for approximate MDP algorithms. We provide experimental results on problems with over 1040 states, demonstrating a promising indication of the scalability of our approach, and compare our algorithm to an existing state-of-the-art approach, showing, in some problems, exponential gains in computation time.},
	language = {en},
	urldate = {2019-01-07},
	journal = {Journal of Artificial Intelligence Research},
	author = {Guestrin, C. and Koller, D. and Parr, R. and Venkataraman, S.},
	month = oct,
	year = {2003},
	pages = {399--468},
	file = {Guestrin et al. - 2003 - Efficient Solution Algorithms for Factored MDPs.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/HLZ3DDVD/Guestrin et al. - 2003 - Efficient Solution Algorithms for Factored MDPs.pdf:application/pdf}
}

@incollection{chentanez_intrinsically_2005,
	title = {Intrinsically {Motivated} {Reinforcement} {Learning}},
	url = {http://papers.nips.cc/paper/2552-intrinsically-motivated-reinforcement-learning.pdf},
	urldate = {2019-01-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 17},
	publisher = {MIT Press},
	author = {Chentanez, Nuttapong and Barto, Andrew G. and Singh, Satinder P.},
	editor = {Saul, L. K. and Weiss, Y. and Bottou, L.},
	year = {2005},
	pages = {1281--1288},
	file = {NIPS Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/P2B7TETY/Chentanez et al. - 2005 - Intrinsically Motivated Reinforcement Learning.pdf:application/pdf;NIPS Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/HFXUPNH7/2552-intrinsically-motivated-reinforcement-learning.html:text/html}
}

@incollection{thrun_finding_1995,
	title = {Finding {Structure} in {Reinforcement} {Learning}},
	url = {http://papers.nips.cc/paper/887-finding-structure-in-reinforcement-learning.pdf},
	urldate = {2019-01-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 7},
	publisher = {MIT Press},
	author = {Thrun, Sebastian and Schwartz, Anton},
	editor = {Tesauro, G. and Touretzky, D. S. and Leen, T. K.},
	year = {1995},
	pages = {385--392},
	file = {NIPS Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/NRG465FT/Thrun and Schwartz - 1995 - Finding Structure in Reinforcement Learning.pdf:application/pdf;NIPS Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/H6MK4RW3/887-finding-structure-in-reinforcement-learning.html:text/html}
}

@article{casanueva_feudal_2018,
	title = {Feudal {Reinforcement} {Learning} for {Dialogue} {Management} in {Large} {Domains}},
	url = {http://arxiv.org/abs/1803.03232},
	abstract = {Reinforcement learning (RL) is a promising approach to solve dialogue policy optimisation. Traditional RL algorithms, however, fail to scale to large domains due to the curse of dimensionality. We propose a novel Dialogue Management architecture, based on Feudal RL, which decomposes the decision into two steps; a first step where a master policy selects a subset of primitive actions, and a second step where a primitive action is chosen from the selected subset. The structural information included in the domain ontology is used to abstract the dialogue state space, taking the decisions at each step using different parts of the abstracted state. This, combined with an information sharing mechanism between slots, increases the scalability to large domains. We show that an implementation of this approach, based on Deep-Q Networks, significantly outperforms previous state of the art in several dialogue domains and environments, without the need of any additional reward signal.},
	urldate = {2019-01-07},
	journal = {arXiv:1803.03232 [cs]},
	author = {Casanueva, Iñigo and Budzianowski, Paweł and Su, Pei-Hao and Ultes, Stefan and Rojas-Barahona, Lina and Tseng, Bo-Hsiang and Gašić, Milica},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.03232},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Accepted as a short paper in NAACL 2018},
	file = {arXiv\:1803.03232 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/8SL9NXA7/Casanueva et al. - 2018 - Feudal Reinforcement Learning for Dialogue Managem.pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/UGGJEUH6/1803.html:text/html}
}

@incollection{moore_parti-game_1994,
	title = {The {Parti}-{Game} {Algorithm} for {Variable} {Resolution} {Reinforcement} {Learning} in {Multidimensional} {State}-{Spaces}},
	url = {http://papers.nips.cc/paper/742-the-parti-game-algorithm-for-variable-resolution-reinforcement-learning-in-multidimensional-state-spaces.pdf},
	urldate = {2019-01-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 6},
	publisher = {Morgan-Kaufmann},
	author = {Moore, Andrew W.},
	editor = {Cowan, J. D. and Tesauro, G. and Alspector, J.},
	year = {1994},
	pages = {711--718},
	file = {NIPS Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/MNJZJ4BY/Moore - 1994 - The Parti-Game Algorithm for Variable Resolution R.pdf:application/pdf;NIPS Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/4D3DH2P8/742-the-parti-game-algorithm-for-variable-resolution-reinforcement-learning-in-multidimensional.html:text/html}
}

@incollection{bradtke_reinforcement_1995,
	title = {Reinforcement {Learning} {Methods} for {Continuous}-{Time} {Markov} {Decision} {Problems}},
	url = {http://papers.nips.cc/paper/889-reinforcement-learning-methods-for-continuous-time-markov-decision-problems.pdf},
	urldate = {2019-01-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 7},
	publisher = {MIT Press},
	author = {Bradtke, Steven J. and Duff, Michael O.},
	editor = {Tesauro, G. and Touretzky, D. S. and Leen, T. K.},
	year = {1995},
	pages = {393--400},
	file = {NIPS Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/VJ4WYJJY/Bradtke and Duff - 1995 - Reinforcement Learning Methods for Continuous-Time.pdf:application/pdf;NIPS Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/SU6RG6V7/889-reinforcement-learning-methods-for-continuous-time-markov-decision-problems.html:text/html}
}

@phdthesis{parr_hierarchical_1998,
	type = {{PhD} {Thesis}},
	title = {Hierarchical {Control} and {Learning} for {Markov} {Decision} {Processes}},
	abstract = {Loading...},
	school = {University of California, Berkeley},
	author = {Parr, Ronald Edward},
	year = {1998},
	annote = {AAI9902197},
	file = {thesis600.ps:/Users/guillaume/Documents/Scolaire/Zotero/storage/LW6NKYZI/thesis600.ps:application/postscript}
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992698},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
	language = {en},
	number = {3},
	urldate = {2019-01-07},
	journal = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	month = may,
	year = {1992},
	keywords = {Q-learning, asynchronous dynamic programming, reinforcement learning, temporal differences},
	pages = {279--292},
	file = {Springer Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/MN4HIIPU/Watkins and Dayan - 1992 - Q-learning.pdf:application/pdf}
}

@article{taylor_transfer_2009,
	title = {Transfer {Learning} for {Reinforcement} {Learning} {Domains}: {A} {Survey}},
	volume = {10},
	issn = {ISSN 1533-7928},
	shorttitle = {Transfer {Learning} for {Reinforcement} {Learning} {Domains}},
	url = {http://www.jmlr.org/papers/v10/taylor09a.html},
	number = {Jul},
	urldate = {2019-01-08},
	journal = {Journal of Machine Learning Research},
	author = {Taylor, Matthew E. and Stone, Peter},
	year = {2009},
	pages = {1633--1685},
	file = {Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/8GTD4SZ3/Taylor and Stone - 2009 - Transfer Learning for Reinforcement Learning Domai.pdf:application/pdf;Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/TSLA86YM/taylor09a.html:text/html}
}

@inproceedings{barto_intrinsically_2004,
	title = {Intrinsically motivated learning of hierarchical collections of skills},
	abstract = {Humans and other animals often engage in activities for their own sakes rather than as steps toward solving practical problems. Psychologists call these intrinsically motivated behaviors. What we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. At the core of the model are recent theoretical and algorithmic advances in computational reinforcement learning, specifically, new concepts related to skills and new learning algorithms for learning with skill hierarchies. 1.},
	author = {Barto, Andrew G.},
	year = {2004},
	pages = {112--119},
	file = {Citeseer - Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/GG3V6D89/Barto - 2004 - Intrinsically motivated learning of hierarchical c.pdf:application/pdf;Citeseer - Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/74LPYXWR/summary.html:text/html}
}

@inproceedings{brunskill_pac-inspired_2014,
	address = {Beijing, China},
	series = {{ICML}'14},
	title = {{PAC}-inspired {Option} {Discovery} in {Lifelong} {Reinforcement} {Learning}},
	url = {http://dl.acm.org/citation.cfm?id=3044805.3044928},
	abstract = {A key goal of AI is to create lifelong learning agents that can leverage prior experience to improve performance on later tasks. In reinforcement-learning problems, one way to summarize prior experience for future use is through options, which are temporally extended actions (subpolicies) for how to behave. Options can then be used to potentially accelerate learning in new reinforcement learning tasks. In this work, we provide the first formal analysis of the sample complexity, a measure of learning speed, of reinforcement learning with options. This analysis helps shed light on some interesting prior empirical results on when and how options may accelerate learning. We then quantify the benefit of options in reducing sample complexity of a lifelong learning agent. Finally, the new theoretical insights inspire a novel option-discovery algorithm that aims at minimizing overall sample complexity in lifelong reinforcement learning.},
	urldate = {2019-01-08},
	booktitle = {Proceedings of the 31st {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 32},
	publisher = {JMLR.org},
	author = {Brunskill, Emma and Li, Lihong},
	year = {2014},
	pages = {II--316--II--324},
	file = {Brunskill and Li - PAC-inspired Option Discovery in Lifelong Reinforc.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/S9IAQCDP/Brunskill and Li - PAC-inspired Option Discovery in Lifelong Reinforc.pdf:application/pdf}
}

@inproceedings{diuk_hierarchical_2006,
	address = {New York, NY, USA},
	series = {{AAMAS} '06},
	title = {A {Hierarchical} {Approach} to {Efficient} {Reinforcement} {Learning} in {Deterministic} {Domains}},
	isbn = {978-1-59593-303-4},
	url = {http://doi.acm.org/10.1145/1160633.1160686},
	doi = {10.1145/1160633.1160686},
	abstract = {Factored representations, model-based learning, and hierarchies are well-studied techniques for improving the learning efficiency of reinforcement-learning algorithms in large-scale state spaces. We bring these three ideas together in a new algorithm. Our algorithm tackles two open problems from the reinforcement-learning literature, and provides a solution to those problems in deterministic domains. First, it shows how models can improve learning speed in the hierarchy-based MaxQ framework without disrupting opportunities for state abstraction. Second, we show how hierarchies can augment existing factored exploration algorithms to achieve not only low sample complexity for learning, but provably efficient planning as well. We illustrate the resulting performance gains in example domains. We prove polynomial bounds on the computational effort needed to attain near optimal performance within the hierarchy.},
	urldate = {2019-01-08},
	booktitle = {Proceedings of the {Fifth} {International} {Joint} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}},
	publisher = {ACM},
	author = {Diuk, Carlos and Strehl, Alexander L. and Littman, Michael L.},
	year = {2006},
	keywords = {reinforcement learning, factored representations, hierarchical reinforcement learning, sample complexity},
	pages = {313--319},
	file = {Citeseer - Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/AJP75RUF/Diuk - 2006 - A hierarchical approach to efficient reinforcement.pdf:application/pdf}
}

@inproceedings{mirzazadeh_new_2007,
	title = {A {New} {Learning} {Algorithm} for the {Maxq} {Hierarchical} {Reinforcement} {Learning} {Method}},
	doi = {10.1109/ICICT.2007.375352},
	abstract = {The MAXQ hierarchical reinforcement learning method is computationally expensive in applications with deep hierarchy. In this paper, we propose a new learning algorithm for MAXQ method to address the open problem of reducing its computational complexity. While the computational cost of the algorithm is considerably decreased, the required storage of new algorithm is less than two times as the original learning algorithm requires storage. Our experimental results in the simple taxi domain problem show satisfactory behavior of the new algorithm.},
	booktitle = {2007 {International} {Conference} on {Information} and {Communication} {Technology}},
	author = {Mirzazadeh, F. and Behsaz, B. and Beigy, H.},
	month = mar,
	year = {2007},
	keywords = {Machine learning, Application software, Communications technology, computational complexity, Computational complexity, Computational efficiency, Computer applications, Finishing, Function approximation, hierarchical reinforcement learning method, Information technology, learning (artificial intelligence), learning algorithm, Learning systems, MAXQ, taxi domain problem},
	pages = {105--108},
	file = {IEEE Xplore Abstract Record:/Users/guillaume/Documents/Scolaire/Zotero/storage/VVP8QIJM/4261375.html:text/html;Submitted Version:/Users/guillaume/Documents/Scolaire/Zotero/storage/3QEZ3XZM/Mirzazadeh et al. - 2007 - A New Learning Algorithm for the Maxq Hierarchical.pdf:application/pdf}
}

@incollection{goos_generating_2000,
	address = {Berlin, Heidelberg},
	title = {Generating {Hierarchical} {Structure} in {Reinforcement} {Learning} from {State} {Variables}},
	volume = {1886},
	isbn = {978-3-540-67925-7 978-3-540-44533-3},
	url = {http://link.springer.com/10.1007/3-540-44533-1_54},
	abstract = {This paper presents the CQ algorithm which decomposes and solves a Markov Decision Process (MDP) by automatically generating a hierarchy of smaller MDPs using state variables. The CQ algorithm uses a heuristic which is applicable for problems that can be modelled by a set of state variables that conform to a special ordering, defined in this paper as a “nested Markov ordering”. The benefits of this approach are: (1) the automatic generation of actions and termination conditions at all levels in the hierarchy, and (2) linear scaling with the number of variables under certain conditions. This approach draws heavily on Dietterich's MAXQ value function decomposition and Hauskrecht, Meuleau, Kaelbling, Dean, Boutilier's and others region based decomposition of MDPs. The CQ algorithm is described and its functionality illustrated using a four room example. Different solutions are generated with different numbers of hierarchical levels to solve Dietterich's taxi tasks.},
	language = {en},
	urldate = {2019-01-08},
	booktitle = {{PRICAI} 2000 {Topics} in {Artificial} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hengst, Bernhard},
	editor = {Goos, G. and Hartmanis, J. and van Leeuwen, J. and Mizoguchi, Riichiro and Slaney, John},
	year = {2000},
	doi = {10.1007/3-540-44533-1_54},
	pages = {533--543},
	file = {Hengst - 2000 - Generating Hierarchical Structure in Reinforcement.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/Y6PAVFTT/Hengst - 2000 - Generating Hierarchical Structure in Reinforcement.pdf:application/pdf}
}

@article{mehta_automatic_2007,
	title = {Automatic {Induction} of {MAXQ} {Hierarchies}},
	language = {en},
	author = {Mehta, Neville and Wynkoop, Mike and Ray, Soumya and Tadepalli, Prasad and Dietterich, Tom},
	year = {2007},
	pages = {5},
	file = {Mehta et al. - Automatic Induction of MAXQ Hierarchies.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/HL54HH67/Mehta et al. - Automatic Induction of MAXQ Hierarchies.pdf:application/pdf}
}

@inproceedings{ryan_using_2002,
	title = {Using abstract models of behaviours to automatically generate reinforcement learning hierarchies},
	abstract = {In this paper we present a hybrid system combining techniques from symbolic planning and reinforcement learning. Planning is used to automatically construct task hierarchies for hierarchical models of the behaviours ’ purpose, and to perform intelligent termination improvement when an executing behaviour is no longer appropriate. Reinforcement learning is used to produce concrete implementations of abstractly defined behaviours and to learn the best possible choice of behaviour when plans are ambiguous. Two new hierarchical reinforcement learning algorithms are presented: Planned Hierarchical Semi-Markov Q-Learning (P-HSMQ), a variant of the HSMQ algorithm (Dietterich, 2000b) which uses plan-built task hierarchies, and Teleo-Reactive Q-Learning (TRQ) a more complex algorithm which implements hierarchical reinforcement learning with teleo-reactive execution semantics (Nilsson, 1994). Each algorithm is demonstrated in a simple grid-world domain. 1.},
	booktitle = {In {Proceedings} of {The} 19th {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann},
	author = {Ryan, Malcolm R. K.},
	year = {2002},
	pages = {522--529},
	file = {Citeseer - Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/NF3ZLMT5/Ryan - 2002 - Using abstract models of behaviours to automatical.pdf:application/pdf;Citeseer - Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/GUDNPH69/summary.html:text/html}
}

@inproceedings{bai_efficient_2017,
	address = {Melbourne, Australia},
	title = {Efficient {Reinforcement} {Learning} with {Hierarchies} of {Machines} by {Leveraging} {Internal} {Transitions}},
	isbn = {978-0-9992411-0-3},
	url = {https://www.ijcai.org/proceedings/2017/196},
	doi = {10.24963/ijcai.2017/196},
	abstract = {In the context of hierarchical reinforcement learning, the idea of hierarchies of abstract machines (HAMs) is to write a partial policy as a set of hierarchical ﬁnite state machines with unspeciﬁed choice states, and use reinforcement learning to learn an optimal completion of this partial policy. Given a HAM with deep hierarchical structure, there often exist many internal transitions where a machine calls another machine with the environment state unchanged. In this paper, we propose a new hierarchical reinforcement learning algorithm that automatically discovers such internal transitions, and shortcircuits them recursively in the computation of Q values. The resulting HAMQ-INT algorithm outperforms the state of the art signiﬁcantly on the benchmark Taxi domain and a much more complex RoboCup Keepaway domain.},
	language = {en},
	urldate = {2019-01-08},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Bai, Aijun and Russell, Stuart},
	month = aug,
	year = {2017},
	pages = {1418--1424},
	file = {Bai and Russell - 2017 - Efficient Reinforcement Learning with Hierarchies .pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/XJKGGNLQ/Bai and Russell - 2017 - Efficient Reinforcement Learning with Hierarchies .pdf:application/pdf}
}

@article{konidaris_transfer_2012,
	title = {Transfer in {Reinforcement} {Learning} via {Shared} {Features}},
	volume = {13},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v13/konidaris12a.html},
	number = {May},
	urldate = {2019-01-08},
	journal = {Journal of Machine Learning Research},
	author = {Konidaris, George and Scheidwasser, Ilya and Barto, Andrew},
	year = {2012},
	pages = {1333--1371},
	file = {Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/RS5UWGUN/Konidaris et al. - 2012 - Transfer in Reinforcement Learning via Shared Feat.pdf:application/pdf;Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/2VB23U4B/konidaris12a.html:text/html}
}

@article{budzianowski_sub-domain_2017,
	title = {Sub-domain {Modelling} for {Dialogue} {Management} with {Hierarchical} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1706.06210},
	abstract = {Human conversation is inherently complex, often spanning many different topics/domains. This makes policy learning for dialogue systems very challenging. Standard flat reinforcement learning methods do not provide an efficient framework for modelling such dialogues. In this paper, we focus on the under-explored problem of multi-domain dialogue management. First, we propose a new method for hierarchical reinforcement learning using the option framework. Next, we show that the proposed architecture learns faster and arrives at a better policy than the existing flat ones do. Moreover, we show how pretrained policies can be adapted to more complex systems with an additional set of new actions. In doing that, we show that our approach has the potential to facilitate policy optimisation for more sophisticated multi-domain dialogue systems.},
	urldate = {2019-01-10},
	journal = {arXiv:1706.06210 [cs]},
	author = {Budzianowski, Paweł and Ultes, Stefan and Su, Pei-Hao and Mrkšić, Nikola and Wen, Tsung-Hsien and Casanueva, Iñigo and Rojas-Barahona, Lina and Gašić, Milica},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.06210},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Update of the section 4 and the bibliography},
	file = {arXiv\:1706.06210 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/238YG7MQ/Budzianowski et al. - 2017 - Sub-domain Modelling for Dialogue Management with .pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/IARC563M/1706.html:text/html}
}

@article{peng_composite_2017,
	title = {Composite {Task}-{Completion} {Dialogue} {Policy} {Learning} via {Hierarchical} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1704.03084},
	abstract = {Building a dialogue agent to fulfill complex tasks, such as travel planning, is challenging because the agent has to learn to collectively complete multiple subtasks. For example, the agent needs to reserve a hotel and book a flight so that there leaves enough time for commute between arrival and hotel check-in. This paper addresses this challenge by formulating the task in the mathematical framework of options over Markov Decision Processes (MDPs), and proposing a hierarchical deep reinforcement learning approach to learning a dialogue manager that operates at different temporal scales. The dialogue manager consists of: (1) a top-level dialogue policy that selects among subtasks or options, (2) a low-level dialogue policy that selects primitive actions to complete the subtask given by the top-level policy, and (3) a global state tracker that helps ensure all cross-subtask constraints be satisfied. Experiments on a travel planning task with simulated and real users show that our approach leads to significant improvements over three baselines, two based on handcrafted rules and the other based on flat deep reinforcement learning.},
	urldate = {2019-01-10},
	journal = {arXiv:1704.03084 [cs]},
	author = {Peng, Baolin and Li, Xiujun and Li, Lihong and Gao, Jianfeng and Celikyilmaz, Asli and Lee, Sungjin and Wong, Kam-Fai},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.03084},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 12 pages, 8 figures},
	file = {arXiv\:1704.03084 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/8NQZCKLP/Peng et al. - 2017 - Composite Task-Completion Dialogue Policy Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/F5IR46GT/1704.html:text/html}
}

@incollection{houthooft_vime:_2016,
	title = {{VIME}: {Variational} {Information} {Maximizing} {Exploration}},
	shorttitle = {{VIME}},
	url = {http://papers.nips.cc/paper/6591-vime-variational-information-maximizing-exploration.pdf},
	urldate = {2019-01-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Houthooft, Rein and Chen, Xi and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {1109--1117},
	file = {NIPS Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/QGRYJK6Y/Houthooft et al. - 2016 - VIME Variational Information Maximizing Explorati.pdf:application/pdf;NIPS Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/TEZNNWBB/6591-vime-variational-information-maximizing-exploration.html:text/html}
}

@incollection{bellemare_unifying_2016,
	title = {Unifying {Count}-{Based} {Exploration} and {Intrinsic} {Motivation}},
	url = {http://papers.nips.cc/paper/6383-unifying-count-based-exploration-and-intrinsic-motivation.pdf},
	urldate = {2019-01-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {1471--1479},
	file = {NIPS Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/G2LE5J75/Bellemare et al. - 2016 - Unifying Count-Based Exploration and Intrinsic Mot.pdf:application/pdf;NIPS Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/HXH29GCT/6383-unifying-count-based-exploration-and-intrinsic-motivation.html:text/html}
}

@article{vigorito_intrinsically_2010,
	title = {Intrinsically {Motivated} {Hierarchical} {Skill} {Learning} in {Structured} {Environments}},
	volume = {2},
	issn = {1943-0604},
	doi = {10.1109/TAMD.2010.2050205},
	abstract = {We present a framework for intrinsically motivated developmental learning of abstract skill hierarchies by reinforcement learning agents in structured environments. Long-term learning of skill hierarchies can drastically improve an agent's efficiency in solving ensembles of related tasks in a complex domain. In structured domains composed of many features, understanding the causal relationships between actions and their effects on different features of the environment can greatly facilitate skill learning. Using Bayesian network structure (learning techniques and structured dynamic programming algorithms), we show that reinforcement learning agents can learn incrementally and autonomously both the causal structure of their environment and a hierarchy of skills that exploit this structure. Furthermore, we present a novel active learning scheme that employs intrinsic motivation to maximize the efficiency with which this structure is learned. As new structure is acquired using an agent's current set of skills, more complex skills are learned, which in turn allow the agent to discover more structure, and so on. This bootstrapping property makes our approach a developmental learning process that results in steadily increasing domain knowledge and behavioral complexity as an agent continues to explore its environment.},
	number = {2},
	journal = {IEEE Transactions on Autonomous Mental Development},
	author = {Vigorito, C. M. and Barto, A. G.},
	month = jun,
	year = {2010},
	keywords = {Machine learning, dynamic programming, reinforcement learning, learning (artificial intelligence), Learning systems, Active learning, Bayesian methods, Bayesian network structure, behavioral complexity, belief networks, Computer science, Context modeling, developmental learning, Dynamic programming, dynamic programming algorithms, Heuristic algorithms, intrinsic motivation, intrinsically motivated hierarchical skill learning, options, planning, reinforcement learning agents, Robustness, State-space methods, structure learning, structured environments},
	pages = {132--143},
	file = {IEEE Xplore Abstract Record:/Users/guillaume/Documents/Scolaire/Zotero/storage/RRKUHLTZ/5464347.html:text/html;Vigorito and Barto - 2010 - Intrinsically Motivated Hierarchical Skill Learnin.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/3VND29Z5/Vigorito and Barto - 2010 - Intrinsically Motivated Hierarchical Skill Learnin.pdf:application/pdf}
}

@article{heess_learning_2016,
	title = {Learning and {Transfer} of {Modulated} {Locomotor} {Controllers}},
	url = {http://arxiv.org/abs/1610.05182},
	abstract = {We study a novel architecture and training procedure for locomotion tasks. A high-frequency, low-level "spinal" network with access to proprioceptive sensors learns sensorimotor primitives by training on simple tasks. This pre-trained module is fixed and connected to a low-frequency, high-level "cortical" network, with access to all sensors, which drives behavior by modulating the inputs to the spinal network. Where a monolithic end-to-end architecture fails completely, learning with a pre-trained spinal module succeeds at multiple high-level tasks, and enables the effective exploration required to learn from sparse rewards. We test our proposed architecture on three simulated bodies: a 16-dimensional swimming snake, a 20-dimensional quadruped, and a 54-dimensional humanoid. Our results are illustrated in the accompanying video at https://youtu.be/sboPYvhpraQ},
	urldate = {2019-01-10},
	journal = {arXiv:1610.05182 [cs]},
	author = {Heess, Nicolas and Wayne, Greg and Tassa, Yuval and Lillicrap, Timothy and Riedmiller, Martin and Silver, David},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.05182},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	annote = {Comment: Supplemental video available at https://youtu.be/sboPYvhpraQ},
	file = {arXiv\:1610.05182 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/WRY3KI9T/Heess et al. - 2016 - Learning and Transfer of Modulated Locomotor Contr.pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/JCTG9P47/1610.html:text/html}
}

@incollection{konidaris_skill_2009,
	title = {Skill {Discovery} in {Continuous} {Reinforcement} {Learning} {Domains} using {Skill} {Chaining}},
	url = {http://papers.nips.cc/paper/3683-skill-discovery-in-continuous-reinforcement-learning-domains-using-skill-chaining.pdf},
	urldate = {2019-01-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Konidaris, George and Barto, Andrew G.},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {1015--1023},
	file = {NIPS Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/WS3TWGTJ/Konidaris and Barto - 2009 - Skill Discovery in Continuous Reinforcement Learni.pdf:application/pdf;NIPS Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/IJAYQJJV/3683-skill-discovery-in-continuous-reinforcement-learning-domains-using-skill-chaining.html:text/html}
}

@article{daniel_probabilistic_2016,
	title = {Probabilistic inference for determining options in reinforcement learning},
	volume = {104},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-016-5580-x},
	doi = {10.1007/s10994-016-5580-x},
	abstract = {Tasks that require many sequential decisions or complex solutions are hard to solve using conventional reinforcement learning algorithms. Based on the semi Markov decision process setting (SMDP) and the option framework, we propose a model which aims to alleviate these concerns. Instead of learning a single monolithic policy, the agent learns a set of simpler sub-policies as well as the initiation and termination probabilities for each of those sub-policies. While existing option learning algorithms frequently require manual specification of components such as the sub-policies, we present an algorithm which infers all relevant components of the option framework from data. Furthermore, the proposed approach is based on parametric option representations and works well in combination with current policy search methods, which are particularly well suited for continuous real-world tasks. We present results on SMDPs with discrete as well as continuous state-action spaces. The results show that the presented algorithm can combine simple sub-policies to solve complex tasks and can improve learning performance on simpler tasks.},
	language = {en},
	number = {2},
	urldate = {2019-01-10},
	journal = {Machine Learning},
	author = {Daniel, Christian and van Hoof, Herke and Peters, Jan and Neumann, Gerhard},
	month = sep,
	year = {2016},
	keywords = {Reinforcement learning, Options, Robot learning, Semi Markov decision process},
	pages = {337--357},
	file = {Springer Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/T72996J9/Daniel et al. - 2016 - Probabilistic inference for determining options in.pdf:application/pdf}
}

@article{mann_approximate_2015,
	title = {Approximate {Value} {Iteration} with {Temporally} {Extended} {Actions}},
	volume = {53},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/10947},
	doi = {10.1613/jair.4676},
	language = {en},
	urldate = {2019-01-10},
	journal = {Journal of Artificial Intelligence Research},
	author = {Mann, Timothy A. and Mannor, Shie and Precup, Doina},
	month = jul,
	year = {2015},
	pages = {375--438},
	file = {Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/NUI53D4T/Mann et al. - 2015 - Approximate Value Iteration with Temporally Extend.pdf:application/pdf;Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/GBFJ2ETI/10947.html:text/html}
}

@incollection{mankowitz_adaptive_2016,
	title = {Adaptive {Skills} {Adaptive} {Partitions} ({ASAP})},
	url = {http://papers.nips.cc/paper/6350-adaptive-skills-adaptive-partitions-asap.pdf},
	urldate = {2019-01-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Mankowitz, Daniel J and Mann, Timothy A and Mannor, Shie},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {1588--1596},
	file = {NIPS Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/67T9L3TU/Mankowitz et al. - 2016 - Adaptive Skills Adaptive Partitions (ASAP).pdf:application/pdf;NIPS Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/FV7BRIBE/6350-adaptive-skills-adaptive-partitions-asap.html:text/html}
}

@article{mann_scaling_2014,
	title = {Scaling {Up} {Approximate} {Value} {Iteration} with {Options}: {Better} {Policies} with {Fewer} {Iterations}},
	abstract = {We show how options, a class of control structures encompassing primitive and temporally extended actions, can play a valuable role in planning in MDPs with continuous state-spaces. Analyzing the convergence rate of Approximate Value Iteration with options reveals that for pessimistic initial value function estimates, options can speed up convergence compared to planning with only primitive actions even when the temporally extended actions are suboptimal and sparsely scattered throughout the state-space. Our experimental results in an optimal replacement task and a complex inventory management task demonstrate the potential for options to speed up convergence in practice. We show that options induce faster convergence to the optimal value function, which implies deriving better policies with fewer iterations.},
	language = {en},
	author = {Mann, Timothy A and Mannor, Shie},
	year = {2014},
	pages = {9},
	file = {Mann and Mannor - Scaling Up Approximate Value Iteration with Option.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/UYYUDJXY/Mann and Mannor - Scaling Up Approximate Value Iteration with Option.pdf:application/pdf}
}

@article{da_silva_learning_2012,
	title = {Learning {Parameterized} {Skills}},
	url = {http://arxiv.org/abs/1206.6398},
	abstract = {We introduce a method for constructing skills capable of solving tasks drawn from a distribution of parameterized reinforcement learning problems. The method draws example tasks from a distribution of interest and uses the corresponding learned policies to estimate the topology of the lower-dimensional piecewise-smooth manifold on which the skill policies lie. This manifold models how policy parameters change as task parameters vary. The method identifies the number of charts that compose the manifold and then applies non-linear regression in each chart to construct a parameterized skill by predicting policy parameters from task parameters. We evaluate our method on an underactuated simulated robotic arm tasked with learning to accurately throw darts at a parameterized target location.},
	urldate = {2019-01-10},
	journal = {arXiv:1206.6398 [cs, stat]},
	author = {Da Silva, Bruno and Konidaris, George and Barto, Andrew},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.6398},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)},
	file = {arXiv\:1206.6398 PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/8IM9BIKV/Da Silva et al. - 2012 - Learning Parameterized Skills.pdf:application/pdf;arXiv.org Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/JQXWJQSC/1206.html:text/html}
}

@inproceedings{jonsson_active_2007,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Active {Learning} of {Dynamic} {Bayesian} {Networks} in {Markov} {Decision} {Processes}},
	isbn = {978-3-540-73580-9},
	abstract = {Several recent techniques for solving Markov decision processes use dynamic Bayesian networks to compactly represent tasks. The dynamic Bayesian network representation may not be given, in which case it is necessary to learn it if one wants to apply these techniques. We develop an algorithm for learning dynamic Bayesian network representations of Markov decision processes using data collected through exploration in the environment. To accelerate data collection we develop a novel scheme for active learning of the networks. We assume that it is not possible to sample the process in arbitrary states, only along trajectories, which prevents us from applying existing active learning techniques. Our active learning scheme selects actions that maximize the total entropy of distributions used to evaluate potential refinements of the networks.},
	language = {en},
	booktitle = {Abstraction, {Reformulation}, and {Approximation}},
	publisher = {Springer Berlin Heidelberg},
	author = {Jonsson, Anders and Barto, Andrew},
	editor = {Miguel, Ian and Ruml, Wheeler},
	year = {2007},
	keywords = {Markov Decision Process, Active Learning, Bayesian Information Criterion, Bayesian Network, Dynamic Bayesian Network},
	pages = {273--284}
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {The theory of reinforcement learning provides a normative account1, deeply rooted in psychological2 and neuroscientific3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems4,5, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms3. While reinforcement learning agents have achieved some successes in a variety of domains6,7,8, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks9,10,11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games12. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
	language = {en},
	number = {7540},
	urldate = {2019-01-10},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533},
	file = {Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/U2L3MST4/nature14236.html:text/html}
}

@incollection{simsek_skill_2009,
	title = {Skill {Characterization} {Based} on {Betweenness}},
	url = {http://papers.nips.cc/paper/3411-skill-characterization-based-on-betweenness.pdf},
	urldate = {2019-01-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 21},
	publisher = {Curran Associates, Inc.},
	author = {Şimşek, Özgür and Barto, Andrew G.},
	editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
	year = {2009},
	pages = {1497--1504},
	file = {NIPS Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/P49RVFSP/Şimşek and Barto - 2009 - Skill Characterization Based on Betweenness.pdf:application/pdf;NIPS Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/4SY4S7RZ/3411-skill-characterization-based-on-betweenness.html:text/html}
}

@inproceedings{diuk_adaptive_2009,
	address = {New York, NY, USA},
	series = {{ICML} '09},
	title = {The {Adaptive} {K}-meteorologists {Problem} and {Its} {Application} to {Structure} {Learning} and {Feature} {Selection} in {Reinforcement} {Learning}},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org/10.1145/1553374.1553406},
	doi = {10.1145/1553374.1553406},
	abstract = {The purpose of this paper is three-fold. First, we formalize and study a problem of learning probabilistic concepts in the recently proposed KWIK framework. We give details of an algorithm, known as the Adaptive k-Meteorologists Algorithm, analyze its sample-complexity upper bound, and give a matching lower bound. Second, this algorithm is used to create a new reinforcement-learning algorithm for factored-state problems that enjoys significant improvement over the previous state-of-the-art algorithm. Finally, we apply the Adaptive k-Meteorologists Algorithm to remove a limiting assumption in an existing reinforcement-learning algorithm. The effectiveness of our approaches is demonstrated empirically in a couple benchmark domains as well as a robotics navigation problem.},
	urldate = {2019-01-10},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Diuk, Carlos and Li, Lihong and Leffler, Bethany R.},
	year = {2009},
	pages = {249--256},
	file = {Submitted Version:/Users/guillaume/Documents/Scolaire/Zotero/storage/NDG54UXA/Diuk et al. - 2009 - The Adaptive K-meteorologists Problem and Its Appl.pdf:application/pdf}
}

@article{lin_self-improving_1992,
	title = {Self-improving reactive agents based on reinforcement learning, planning and teaching},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992699},
	doi = {10.1007/BF00992699},
	abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.This paper compares eight reinforcement learning frameworks:adaptive heuristic critic (AHC) learning due to Sutton,Q-learning due to Watkins, and three extensions to both basic methods for speeding up learning. The three extensions are experience replay, learning action models for planning, and teaching. The frameworks were investigated using connectionism as an approach to generalization. To evaluate the performance of different frameworks, a dynamic environment was used as a testbed. The environment is moderately complex and nondeterministic. This paper describes these frameworks and algorithms in detail and presents empirical evaluation of the frameworks.},
	language = {en},
	number = {3},
	urldate = {2019-01-10},
	journal = {Machine Learning},
	author = {Lin, Long-Ji},
	month = may,
	year = {1992},
	keywords = {Reinforcement learning, planning, connectionist networks, teaching},
	pages = {293--321},
	file = {Springer Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/IMAHNSWJ/Lin - 1992 - Self-improving reactive agents based on reinforcem.pdf:application/pdf}
}

@incollection{mellouk_subgoal_2011,
	title = {Subgoal {Identifications} in {Reinforcement} {Learning}: {A} {Survey}},
	isbn = {978-953-307-369-9},
	shorttitle = {Subgoal {Identifications} in {Reinforcement} {Learning}},
	url = {http://www.intechopen.com/books/advances-in-reinforcement-learning/subgoal-identifications-in-reinforcement-learning-a-survey},
	language = {en},
	urldate = {2019-01-10},
	booktitle = {Advances in {Reinforcement} {Learning}},
	publisher = {InTech},
	author = {Chiu, Chung-Cheng and Soo, Von-Wun},
	editor = {Mellouk, Abdelhamid},
	month = jan,
	year = {2011},
	doi = {10.5772/13214},
	file = {Chiu and Soo - 2011 - Subgoal Identifications in Reinforcement Learning.pdf:/Users/guillaume/Documents/Scolaire/Zotero/storage/XL92NL4H/Chiu and Soo - 2011 - Subgoal Identifications in Reinforcement Learning.pdf:application/pdf}
}

@inproceedings{pickett_policyblocks:_2002,
	title = {{PolicyBlocks}: {An} {Algorithm} for {Creating} {Useful} {Macro}-{Actions} in {Reinforcement} {Learning}},
	shorttitle = {{PolicyBlocks}},
	abstract = {We present PolicyBlocks, an algorithm by which a reinforcement learning agent can extract useful macro-actions from a set of related tasks. The agent creates macroactions by finding commonalities in solutions to previous tasks. Using these macro-actions, learning to do future related tasks is accelerated.},
	booktitle = {Proceedings of the {Nineteenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann},
	author = {Pickett, Marc and Barto, Andrew G.},
	year = {2002},
	pages = {506--513},
	file = {Citeseer - Full Text PDF:/Users/guillaume/Documents/Scolaire/Zotero/storage/4IM2Q6X8/Pickett and Barto - 2002 - PolicyBlocks An Algorithm for Creating Useful Mac.pdf:application/pdf;Citeseer - Snapshot:/Users/guillaume/Documents/Scolaire/Zotero/storage/LFWX7DZ3/summary.html:text/html}
}

@inproceedings{jonsson_causal_2005,
	address = {New York, NY, USA},
	series = {{ICML} '05},
	title = {A {Causal} {Approach} to {Hierarchical} {Decomposition} of {Factored} {MDPs}},
	isbn = {978-1-59593-180-1},
	url = {http://doi.acm.org/10.1145/1102351.1102402},
	doi = {10.1145/1102351.1102402},
	abstract = {We present Variable Influence Structure Analysis, an algorithm that dynamically performs hierarchical decomposition of factored Markov decision processes. Our algorithm determines causal relationships between state variables and introduces temporally-extended actions that cause the values of state variables to change. Each temporally-extended action corresponds to a subtask that is significantly easier to solve than the overall task. Results from experiments show great promise in scaling to larger tasks.},
	urldate = {2019-01-15},
	booktitle = {Proceedings of the 22Nd {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Jonsson, Anders and Barto, Andrew},
	year = {2005},
	pages = {401--408},
	file = {Submitted Version:/Users/guillaume/Documents/Scolaire/Zotero/storage/WFSCE6GV/Jonsson and Barto - 2005 - A Causal Approach to Hierarchical Decomposition of.pdf:application/pdf}
}